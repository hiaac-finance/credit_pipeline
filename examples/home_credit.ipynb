{"cells":[{"cell_type":"markdown","metadata":{"id":"AOPCGcD-K-B0"},"source":["# Experiments with Home Credit"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GS4_XnYslJ3J"},"outputs":[{"name":"stderr","output_type":"stream","text":["UsageError: Line magic function `%` not found.\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import roc_auc_score, balanced_accuracy_score, accuracy_score, precision_score, recall_score, f1_score\n","\n","import credit_pipeline.data_exploration as dex\n","from credit_pipeline.training import *\n","\n","% load_ext autoreload\n","% autoreload 2"]},{"cell_type":"markdown","metadata":{},"source":["## Loading and Cleaning"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["seed_number = 0"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["path = \"../data/HomeCredit/\"\n","df = dex.read_csv_encoded(path, 'application_train.csv').sample(10000)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["['COMMONAREA_MODE',\n"," 'COMMONAREA_AVG',\n"," 'COMMONAREA_MEDI',\n"," 'NONLIVINGAPARTMENTS_MODE',\n"," 'NONLIVINGAPARTMENTS_AVG',\n"," 'NONLIVINGAPARTMENTS_MEDI',\n"," 'FONDKAPREMONT_MODE',\n"," 'LIVINGAPARTMENTS_MEDI',\n"," 'LIVINGAPARTMENTS_AVG',\n"," 'LIVINGAPARTMENTS_MODE',\n"," 'FLOORSMIN_MODE',\n"," 'FLOORSMIN_AVG',\n"," 'FLOORSMIN_MEDI',\n"," 'YEARS_BUILD_MEDI',\n"," 'YEARS_BUILD_MODE',\n"," 'YEARS_BUILD_AVG',\n"," 'OWN_CAR_AGE',\n"," 'LANDAREA_MODE',\n"," 'LANDAREA_MEDI',\n"," 'LANDAREA_AVG',\n"," 'BASEMENTAREA_AVG',\n"," 'BASEMENTAREA_MODE',\n"," 'BASEMENTAREA_MEDI',\n"," 'EXT_SOURCE_1',\n"," 'NONLIVINGAREA_MEDI',\n"," 'NONLIVINGAREA_AVG',\n"," 'NONLIVINGAREA_MODE',\n"," 'ELEVATORS_MODE',\n"," 'ELEVATORS_MEDI',\n"," 'ELEVATORS_AVG',\n"," 'APARTMENTS_MODE',\n"," 'APARTMENTS_MEDI',\n"," 'APARTMENTS_AVG',\n"," 'WALLSMATERIAL_MODE',\n"," 'ENTRANCES_AVG',\n"," 'ENTRANCES_MODE',\n"," 'ENTRANCES_MEDI',\n"," 'HOUSETYPE_MODE',\n"," 'LIVINGAREA_AVG',\n"," 'LIVINGAREA_MODE',\n"," 'LIVINGAREA_MEDI',\n"," 'FLOORSMAX_MODE',\n"," 'FLOORSMAX_AVG',\n"," 'FLOORSMAX_MEDI']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["columns_to_drop = dex.check_missing(df, 50,  False)\n","columns_to_drop"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df = df.drop(columns_to_drop, axis=1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["['NAME_CONTRACT_TYPE',\n"," 'CODE_GENDER',\n"," 'FLAG_OWN_CAR',\n"," 'FLAG_OWN_REALTY',\n"," 'NAME_TYPE_SUITE',\n"," 'NAME_INCOME_TYPE',\n"," 'NAME_EDUCATION_TYPE',\n"," 'NAME_FAMILY_STATUS',\n"," 'NAME_HOUSING_TYPE',\n"," 'OCCUPATION_TYPE',\n"," 'WEEKDAY_APPR_PROCESS_START',\n"," 'ORGANIZATION_TYPE',\n"," 'EMERGENCYSTATE_MODE']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_cols = df.columns.to_list()\n","obj_cols = dex.list_by_type(df, ['O'])\n","obj_cols"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def days_to_years(dataframe, col_name):\n","        \"\"\"\n","        Converts values from string to numeric.\n","        Uses the map function to convert the information on days employed to years employed\n","        \"\"\"\n","        df_name = dataframe.copy()\n","\n","        if col_name in df.columns:\n","            #Converts values from string to numeric.\n","            df_name[col_name] = pd.to_numeric(df_name[col_name], errors='coerce')\n","\n","            #drops null values on the column\n","            df_name = df_name.dropna(subset=[col_name])\n","\n","            #Use the map function to convert the information on days employed to years employed\n","            year = df_name.loc[:, col_name].map(lambda x: int(abs(x / 365)), na_action=None)\n","            df_name['YEARS'+col_name[4:]] = year\n","\n","            #drops the column\n","            df_name = df_name.drop(col_name, axis=1)\n","\n","        return df_name\n","\n","\n","df = days_to_years(df, \"DAYS_EMPLOYED\")\n","df = days_to_years(df, 'DAYS_BIRTH')"]},{"cell_type":"markdown","metadata":{},"source":["## Training Basic Models"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"chM4tJtz9V6e"},"outputs":[],"source":["X_acp = df.iloc[:, (df.columns != \"TARGET\") & (df.columns != \"SK_ID_CURR\")]\n","y_acp = df[\"TARGET\"]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"9blMaysuPui4"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X_acp, \n","    y_acp, \n","    test_size = 0.2, \n","    random_state=seed_number, \n","    stratify=y_acp\n",")\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, \n","    y_train, \n","    test_size = 0.2, \n","    random_state=seed_number, \n","    stratify=y_train\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1695152363955,"user":{"displayName":"Athyrson Machado Ribeiro","userId":"03264544371961652552"},"user_tz":180},"id":"FP4M_FYobzNM","outputId":"c6968a46-e41f-40bd-9bfe-50021697f066"},"outputs":[],"source":["classifiers = {\n","    \"Logistic Regression\": LogisticRegression,\n","    \"Random Forest\": RandomForestClassifier,\n","    \"LightGBM\": LGBMClassifier\n","}"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Gf0vyh7ec9vi"},"outputs":[],"source":["param_spaces = {\n","    \"LogisticRegression\": {\n","        'C': {'low': 0.001, 'high': 10, 'log': True, 'type':'float'},\n","        'max_iter': {'low': 1000, 'high': 1000, 'step':1, 'type':'int'},\n","        'penalty': {'choices': ['l2'], 'type':'categorical'}\n","    },\n","    \"RandomForestClassifier\": {\n","        'n_estimators': {'low':10, 'high':150, 'step':1, 'type':'int'},\n","        'max_depth': {'low':2, 'high':10, 'type':'int'},\n","        'criterion': {'choices':['gini', 'entropy'], 'type':'categorical'}\n","    },\n","    \"LGBMClassifier\": {\n","        'learning_rate': {'low': 0.01, 'high': 1.0, 'type': 'float', 'log': True},\n","        'num_leaves': {'low': 2, 'high': 15, 'step': 1, 'type': 'int'},\n","        'max_depth': {'low': 1, 'high': 10, 'type': 'int'},\n","        'min_child_samples': {'low': 1, 'high': 50, 'step': 1, 'type': 'int'},\n","        'subsample': {'low': 0.1, 'high': 1.0, 'type': 'float'},\n","        'colsample_bytree': {'low': 0.1, 'high': 1.0, 'type': 'float'},\n","        'reg_alpha': {'low': 0.0, 'high': 0.5, 'type': 'float'},\n","        'reg_lambda': {'low': 0.0, 'high': 1.0, 'type': 'float'},\n","        # 'n_estimators': {'low': 50, 'high': 200, 'step': 1, 'type': 'int'},\n","        #'verbose': {'low':-1, 'high': -1, 'type': 'int'}\n","    }\n","}"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:54:30,305] A new study created in memory with name: no-name-018f735a-3d08-4386-965d-fe3e843c0c3e\n","[I 2023-09-27 18:54:31,400] Trial 0 finished with value: 0.7420801492483765 and parameters: {'C': 0.15676677195506075, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 0 with value: 0.7420801492483765.\n","[I 2023-09-27 18:54:31,721] Trial 1 finished with value: 0.742961698725341 and parameters: {'C': 0.7257005721594281, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 1 with value: 0.742961698725341.\n","[I 2023-09-27 18:54:31,955] Trial 2 finished with value: 0.7425824274387399 and parameters: {'C': 0.25766385746135895, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 1 with value: 0.742961698725341.\n","[I 2023-09-27 18:54:32,287] Trial 3 finished with value: 0.7419571423446141 and parameters: {'C': 0.15119336467641012, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 1 with value: 0.742961698725341.\n","[I 2023-09-27 18:54:32,527] Trial 4 finished with value: 0.7399172778572196 and parameters: {'C': 0.04950159553733195, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 1 with value: 0.742961698725341.\n","[I 2023-09-27 18:54:32,766] Trial 5 finished with value: 0.743161584943955 and parameters: {'C': 0.3833332156156664, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:33,013] Trial 6 finished with value: 0.7401222893634904 and parameters: {'C': 0.0562793204741517, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:33,355] Trial 7 finished with value: 0.7383335639712779 and parameters: {'C': 3.6905577292137624, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:33,739] Trial 8 finished with value: 0.7359093029096259 and parameters: {'C': 7.155682161754871, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:33,955] Trial 9 finished with value: 0.7395226307076485 and parameters: {'C': 0.03417952912061012, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:34,247] Trial 10 finished with value: 0.7343102131607137 and parameters: {'C': 0.0034975207958333164, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:34,498] Trial 11 finished with value: 0.7423876665077828 and parameters: {'C': 1.144606648807925, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:34,753] Trial 12 finished with value: 0.7429104458487732 and parameters: {'C': 0.863185219688138, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:35,037] Trial 13 finished with value: 0.7428899446981462 and parameters: {'C': 0.7777512402254558, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:35,385] Trial 14 finished with value: 0.7406706951427651 and parameters: {'C': 2.050531660406252, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:35,741] Trial 15 finished with value: 0.7351405097611102 and parameters: {'C': 8.455970159909459, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:36,099] Trial 16 finished with value: 0.7430590791908196 and parameters: {'C': 0.6745973544735692, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:36,377] Trial 17 finished with value: 0.7431052067797306 and parameters: {'C': 0.3341080705306469, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:36,612] Trial 18 finished with value: 0.742802814807981 and parameters: {'C': 0.2856789897912428, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:36,913] Trial 19 finished with value: 0.7397173916386057 and parameters: {'C': 2.758834787857237, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:37,133] Trial 20 finished with value: 0.7388358421616413 and parameters: {'C': 0.0180500307466597, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:37,368] Trial 21 finished with value: 0.7431462090809847 and parameters: {'C': 0.3862054668281112, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:37,612] Trial 22 finished with value: 0.743053953903163 and parameters: {'C': 0.32023273680990816, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:37,833] Trial 23 finished with value: 0.7412652285109501 and parameters: {'C': 0.09805384977324962, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:38,144] Trial 24 finished with value: 0.7431205826427008 and parameters: {'C': 0.35223483889794266, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:38,385] Trial 25 finished with value: 0.7431410837933277 and parameters: {'C': 0.38785469219271185, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:38,808] Trial 26 finished with value: 0.7415009917431616 and parameters: {'C': 1.5747150623954111, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:39,071] Trial 27 finished with value: 0.7414497388665938 and parameters: {'C': 0.1148270348842234, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 5 with value: 0.743161584943955.\n","[I 2023-09-27 18:54:39,335] Trial 28 finished with value: 0.7431718355192686 and parameters: {'C': 0.47817578730589233, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 28 with value: 0.7431718355192686.\n","[I 2023-09-27 18:54:39,634] Trial 29 finished with value: 0.7414241124283099 and parameters: {'C': 1.6953325045649505, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 28 with value: 0.7431718355192686.\n","[I 2023-09-27 18:54:39,912] Trial 30 finished with value: 0.7423517894941855 and parameters: {'C': 0.18898666057148839, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 28 with value: 0.7431718355192686.\n","[I 2023-09-27 18:54:40,225] Trial 31 finished with value: 0.7431769608069253 and parameters: {'C': 0.6099767434554221, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 31 with value: 0.7431769608069253.\n","[I 2023-09-27 18:54:40,528] Trial 32 finished with value: 0.742936072287057 and parameters: {'C': 0.7138284775294665, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 31 with value: 0.7431769608069253.\n","[I 2023-09-27 18:54:40,791] Trial 33 finished with value: 0.7432025872452092 and parameters: {'C': 0.5117952155135288, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 33 with value: 0.7432025872452092.\n","[I 2023-09-27 18:54:41,047] Trial 34 finished with value: 0.743115457355044 and parameters: {'C': 0.5734623907037631, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 33 with value: 0.7432025872452092.\n","[I 2023-09-27 18:54:41,299] Trial 35 finished with value: 0.7423210377682447 and parameters: {'C': 0.1845936359122024, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 33 with value: 0.7432025872452092.\n","[I 2023-09-27 18:54:41,568] Trial 36 finished with value: 0.7423517894941853 and parameters: {'C': 1.1195220044779197, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 33 with value: 0.7432025872452092.\n","[I 2023-09-27 18:54:41,855] Trial 37 finished with value: 0.7432435895464633 and parameters: {'C': 0.46243104890204667, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:42,117] Trial 38 finished with value: 0.743207712532866 and parameters: {'C': 0.5147778713251056, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:42,362] Trial 39 finished with value: 0.7423722906448125 and parameters: {'C': 0.19726783710258244, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:42,637] Trial 40 finished with value: 0.7424132929460665 and parameters: {'C': 1.1237543707273343, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:42,884] Trial 41 finished with value: 0.7431359585056712 and parameters: {'C': 0.5644178265197617, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:43,138] Trial 42 finished with value: 0.7432179631081793 and parameters: {'C': 0.5349347066754165, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:43,472] Trial 43 finished with value: 0.7384360697244133 and parameters: {'C': 3.546347031295534, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:43,708] Trial 44 finished with value: 0.742628555027651 and parameters: {'C': 0.2370368668995359, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:43,966] Trial 45 finished with value: 0.7431769608069253 and parameters: {'C': 0.5505441489122141, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:44,270] Trial 46 finished with value: 0.7426388056029644 and parameters: {'C': 0.991105380398379, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:44,531] Trial 47 finished with value: 0.7419571423446141 and parameters: {'C': 1.3809770876030847, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:44,844] Trial 48 finished with value: 0.7402760479931938 and parameters: {'C': 2.2708033604019464, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:45,153] Trial 49 finished with value: 0.7428540676845488 and parameters: {'C': 0.8709684176561195, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:45,530] Trial 50 finished with value: 0.737016365043488 and parameters: {'C': 5.177462784293074, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:45,796] Trial 51 finished with value: 0.7432179631081796 and parameters: {'C': 0.5136960613551597, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:46,105] Trial 52 finished with value: 0.742961698725341 and parameters: {'C': 0.7258835389392357, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:46,379] Trial 53 finished with value: 0.7426285550276509 and parameters: {'C': 0.2412266344513243, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 37 with value: 0.7432435895464633.\n","[I 2023-09-27 18:54:46,598] Trial 54 finished with value: 0.743253840121777 and parameters: {'C': 0.43134393856116965, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 54 with value: 0.743253840121777.\n","[I 2023-09-27 18:54:46,812] Trial 55 finished with value: 0.7432743412724038 and parameters: {'C': 0.46350986702622643, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:47,068] Trial 56 finished with value: 0.7415778710580131 and parameters: {'C': 0.12685621436553243, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:47,383] Trial 57 finished with value: 0.742802814807981 and parameters: {'C': 0.27426907647561055, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:47,635] Trial 58 finished with value: 0.7431923366698956 and parameters: {'C': 0.41360160076613317, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:47,895] Trial 59 finished with value: 0.7418648871667922 and parameters: {'C': 1.4237443611096596, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:48,147] Trial 60 finished with value: 0.7428438171092352 and parameters: {'C': 0.8813914344766938, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:48,369] Trial 61 finished with value: 0.7432435895464631 and parameters: {'C': 0.4381154754270892, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:48,587] Trial 62 finished with value: 0.7431718355192686 and parameters: {'C': 0.40000602158767445, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:48,809] Trial 63 finished with value: 0.7429053205611165 and parameters: {'C': 0.2974510081862321, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:49,041] Trial 64 finished with value: 0.7432538401217768 and parameters: {'C': 0.44873520386123994, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:49,302] Trial 65 finished with value: 0.7428438171092353 and parameters: {'C': 0.8108371412020743, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:49,562] Trial 66 finished with value: 0.7422236573027662 and parameters: {'C': 0.16254038041808722, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:49,782] Trial 67 finished with value: 0.7431513343686413 and parameters: {'C': 0.3364355593732907, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:50,010] Trial 68 finished with value: 0.7432384642588065 and parameters: {'C': 0.4430638006424215, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:50,228] Trial 69 finished with value: 0.7425721768634265 and parameters: {'C': 0.2338529763320494, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:50,452] Trial 70 finished with value: 0.7418290101531949 and parameters: {'C': 0.14084665837691598, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:50,701] Trial 71 finished with value: 0.7432230883958363 and parameters: {'C': 0.4307577940958278, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 55 with value: 0.7432743412724038.\n","[I 2023-09-27 18:54:51,003] Trial 72 finished with value: 0.7433050929983446 and parameters: {'C': 0.43193970680315724, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:51,292] Trial 73 finished with value: 0.7431308332180143 and parameters: {'C': 0.37690862601137737, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:51,589] Trial 74 finished with value: 0.7430488286155059 and parameters: {'C': 0.6887192625219685, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:51,873] Trial 75 finished with value: 0.7428950699858029 and parameters: {'C': 0.29929870428927025, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:52,107] Trial 76 finished with value: 0.7431205826427009 and parameters: {'C': 0.3969063090817266, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:52,308] Trial 77 finished with value: 0.741244727360323 and parameters: {'C': 0.0892560969153845, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:52,507] Trial 78 finished with value: 0.7424594205349776 and parameters: {'C': 0.21914132096224453, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:52,804] Trial 79 finished with value: 0.7424542952473207 and parameters: {'C': 1.0896044752262126, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:53,103] Trial 80 finished with value: 0.7431564596562982 and parameters: {'C': 0.66479011260118, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:53,417] Trial 81 finished with value: 0.7432538401217768 and parameters: {'C': 0.4450840905882225, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:53,653] Trial 82 finished with value: 0.743161584943955 and parameters: {'C': 0.4112289144173678, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:53,865] Trial 83 finished with value: 0.7428233159586081 and parameters: {'C': 0.2865360021619824, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:54,136] Trial 84 finished with value: 0.7422595343163635 and parameters: {'C': 0.17875372573661183, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:54,398] Trial 85 finished with value: 0.7432128378205225 and parameters: {'C': 0.6269433057936212, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:54,669] Trial 86 finished with value: 0.742869443547519 and parameters: {'C': 0.8129814165584253, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:54,892] Trial 87 finished with value: 0.743048828615506 and parameters: {'C': 0.3210209840040549, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:55,117] Trial 88 finished with value: 0.74324871483412 and parameters: {'C': 0.4628504832588666, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:55,342] Trial 89 finished with value: 0.7431308332180144 and parameters: {'C': 0.4881209894911229, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:55,609] Trial 90 finished with value: 0.742069898673063 and parameters: {'C': 1.252843839764474, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:55,867] Trial 91 finished with value: 0.74324871483412 and parameters: {'C': 0.4617685960101993, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:56,080] Trial 92 finished with value: 0.7431257079303575 and parameters: {'C': 0.4845786528291776, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:56,392] Trial 93 finished with value: 0.742623429739994 and parameters: {'C': 0.9987159319143155, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:56,639] Trial 94 finished with value: 0.7431718355192686 and parameters: {'C': 0.6216670696814156, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:56,865] Trial 95 finished with value: 0.7426336803153077 and parameters: {'C': 0.2612693482137294, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:57,086] Trial 96 finished with value: 0.7431154573550441 and parameters: {'C': 0.3308904203526438, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:57,303] Trial 97 finished with value: 0.7423620400694988 and parameters: {'C': 0.2075962083587179, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:57,530] Trial 98 finished with value: 0.7432538401217768 and parameters: {'C': 0.4575319999658177, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:57,772] Trial 99 finished with value: 0.742982199875968 and parameters: {'C': 0.7315693403771778, 'max_iter': 1000, 'penalty': 'l2'}. Best is trial 72 with value: 0.7433050929983446.\n","[I 2023-09-27 18:54:57,775] A new study created in memory with name: no-name-72c84b62-7740-4651-8b98-7203af0c569f\n","[I 2023-09-27 18:54:58,611] Trial 0 finished with value: 0.7181399306036051 and parameters: {'n_estimators': 87, 'max_depth': 8, 'criterion': 'gini'}. Best is trial 0 with value: 0.7181399306036051.\n","[I 2023-09-27 18:54:59,217] Trial 1 finished with value: 0.7220043974968096 and parameters: {'n_estimators': 69, 'max_depth': 7, 'criterion': 'entropy'}. Best is trial 1 with value: 0.7220043974968096.\n","[I 2023-09-27 18:55:00,099] Trial 2 finished with value: 0.7170328684697428 and parameters: {'n_estimators': 145, 'max_depth': 5, 'criterion': 'gini'}. Best is trial 1 with value: 0.7220043974968096.\n","[I 2023-09-27 18:55:01,151] Trial 3 finished with value: 0.7137321832187831 and parameters: {'n_estimators': 90, 'max_depth': 10, 'criterion': 'entropy'}. Best is trial 1 with value: 0.7220043974968096.\n","[I 2023-09-27 18:55:01,560] Trial 4 finished with value: 0.6663232723936632 and parameters: {'n_estimators': 12, 'max_depth': 9, 'criterion': 'entropy'}. Best is trial 1 with value: 0.7220043974968096.\n","[I 2023-09-27 18:55:03,038] Trial 5 finished with value: 0.7202771755564782 and parameters: {'n_estimators': 147, 'max_depth': 9, 'criterion': 'entropy'}. Best is trial 1 with value: 0.7220043974968096.\n","[I 2023-09-27 18:55:03,452] Trial 6 finished with value: 0.7181758076172023 and parameters: {'n_estimators': 26, 'max_depth': 7, 'criterion': 'entropy'}. Best is trial 1 with value: 0.7220043974968096.\n","[I 2023-09-27 18:55:04,006] Trial 7 finished with value: 0.7273705736734474 and parameters: {'n_estimators': 83, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:04,700] Trial 8 finished with value: 0.7234394780407051 and parameters: {'n_estimators': 74, 'max_depth': 7, 'criterion': 'entropy'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:05,479] Trial 9 finished with value: 0.7120715900179897 and parameters: {'n_estimators': 96, 'max_depth': 7, 'criterion': 'gini'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:05,834] Trial 10 finished with value: 0.7145189148740974 and parameters: {'n_estimators': 50, 'max_depth': 2, 'criterion': 'gini'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:06,525] Trial 11 finished with value: 0.7250129413513332 and parameters: {'n_estimators': 111, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:07,195] Trial 12 finished with value: 0.7243979068325209 and parameters: {'n_estimators': 118, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:07,808] Trial 13 finished with value: 0.7178734156454532 and parameters: {'n_estimators': 115, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:08,660] Trial 14 finished with value: 0.7254485908021588 and parameters: {'n_estimators': 116, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 7 with value: 0.7273705736734474.\n","[I 2023-09-27 18:55:09,141] Trial 15 finished with value: 0.7302766117748358 and parameters: {'n_estimators': 53, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 15 with value: 0.7302766117748358.\n","[I 2023-09-27 18:55:09,529] Trial 16 finished with value: 0.7303996186785984 and parameters: {'n_estimators': 51, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 16 with value: 0.7303996186785984.\n","[I 2023-09-27 18:55:10,000] Trial 17 finished with value: 0.7239930091076361 and parameters: {'n_estimators': 51, 'max_depth': 6, 'criterion': 'entropy'}. Best is trial 16 with value: 0.7303996186785984.\n","[I 2023-09-27 18:55:10,295] Trial 18 finished with value: 0.7126866245368021 and parameters: {'n_estimators': 47, 'max_depth': 2, 'criterion': 'gini'}. Best is trial 16 with value: 0.7303996186785984.\n","[I 2023-09-27 18:55:10,742] Trial 19 finished with value: 0.7315681842643419 and parameters: {'n_estimators': 62, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 19 with value: 0.7315681842643419.\n","[I 2023-09-27 18:55:11,023] Trial 20 finished with value: 0.7229833274392525 and parameters: {'n_estimators': 30, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 19 with value: 0.7315681842643419.\n","[I 2023-09-27 18:55:11,457] Trial 21 finished with value: 0.7328341303155639 and parameters: {'n_estimators': 64, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 21 with value: 0.7328341303155639.\n","[I 2023-09-27 18:55:11,959] Trial 22 finished with value: 0.7333825360948383 and parameters: {'n_estimators': 65, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 22 with value: 0.7333825360948383.\n","[I 2023-09-27 18:55:12,385] Trial 23 finished with value: 0.7197133939142333 and parameters: {'n_estimators': 67, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 22 with value: 0.7333825360948383.\n","[I 2023-09-27 18:55:12,833] Trial 24 finished with value: 0.734325589023684 and parameters: {'n_estimators': 66, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:13,180] Trial 25 finished with value: 0.7338386866962908 and parameters: {'n_estimators': 35, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:13,580] Trial 26 finished with value: 0.7104776255567343 and parameters: {'n_estimators': 38, 'max_depth': 6, 'criterion': 'gini'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:13,799] Trial 27 finished with value: 0.7148981861606982 and parameters: {'n_estimators': 12, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:14,082] Trial 28 finished with value: 0.7297743335844724 and parameters: {'n_estimators': 30, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:14,546] Trial 29 finished with value: 0.7115641865399696 and parameters: {'n_estimators': 39, 'max_depth': 6, 'criterion': 'gini'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:14,848] Trial 30 finished with value: 0.7121100296754155 and parameters: {'n_estimators': 22, 'max_depth': 2, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:15,437] Trial 31 finished with value: 0.7309736508961565 and parameters: {'n_estimators': 61, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:15,926] Trial 32 finished with value: 0.7200414123242667 and parameters: {'n_estimators': 76, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:16,603] Trial 33 finished with value: 0.7315681842643419 and parameters: {'n_estimators': 62, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:17,408] Trial 34 finished with value: 0.7263967690186611 and parameters: {'n_estimators': 98, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:18,094] Trial 35 finished with value: 0.722773190645325 and parameters: {'n_estimators': 88, 'max_depth': 6, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:18,396] Trial 36 finished with value: 0.7267350380040078 and parameters: {'n_estimators': 41, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:18,863] Trial 37 finished with value: 0.7326086176586661 and parameters: {'n_estimators': 72, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:19,406] Trial 38 finished with value: 0.7144343476277606 and parameters: {'n_estimators': 60, 'max_depth': 8, 'criterion': 'gini'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:19,814] Trial 39 finished with value: 0.7114770566498045 and parameters: {'n_estimators': 78, 'max_depth': 2, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:20,999] Trial 40 finished with value: 0.7212151031976669 and parameters: {'n_estimators': 137, 'max_depth': 10, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:21,452] Trial 41 finished with value: 0.7326854969735176 and parameters: {'n_estimators': 71, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:21,934] Trial 42 finished with value: 0.730763514102229 and parameters: {'n_estimators': 68, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:22,420] Trial 43 finished with value: 0.7311222842382029 and parameters: {'n_estimators': 83, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:23,015] Trial 44 finished with value: 0.7272629426326552 and parameters: {'n_estimators': 96, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:23,383] Trial 45 finished with value: 0.7198927789822203 and parameters: {'n_estimators': 57, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 24 with value: 0.734325589023684.\n","[I 2023-09-27 18:55:23,728] Trial 46 finished with value: 0.7350021269943776 and parameters: {'n_estimators': 44, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 46 with value: 0.7350021269943776.\n","[I 2023-09-27 18:55:24,010] Trial 47 finished with value: 0.7146803614352855 and parameters: {'n_estimators': 23, 'max_depth': 5, 'criterion': 'gini'}. Best is trial 46 with value: 0.7350021269943776.\n","[I 2023-09-27 18:55:24,333] Trial 48 finished with value: 0.7233010952739722 and parameters: {'n_estimators': 46, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 46 with value: 0.7350021269943776.\n","[I 2023-09-27 18:55:24,647] Trial 49 finished with value: 0.7353403959797244 and parameters: {'n_estimators': 36, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:24,907] Trial 50 finished with value: 0.7320960888929892 and parameters: {'n_estimators': 15, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:25,234] Trial 51 finished with value: 0.7300254726796541 and parameters: {'n_estimators': 34, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:25,582] Trial 52 finished with value: 0.7350021269943776 and parameters: {'n_estimators': 44, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:25,929] Trial 53 finished with value: 0.7325266130561576 and parameters: {'n_estimators': 46, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:26,298] Trial 54 finished with value: 0.7204770617750921 and parameters: {'n_estimators': 55, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:26,667] Trial 55 finished with value: 0.734325589023684 and parameters: {'n_estimators': 43, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:27,078] Trial 56 finished with value: 0.734325589023684 and parameters: {'n_estimators': 43, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:27,486] Trial 57 finished with value: 0.7231678377948962 and parameters: {'n_estimators': 45, 'max_depth': 6, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:27,741] Trial 58 finished with value: 0.7144727872851864 and parameters: {'n_estimators': 19, 'max_depth': 5, 'criterion': 'gini'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:28,069] Trial 59 finished with value: 0.7307173865133181 and parameters: {'n_estimators': 29, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:28,471] Trial 60 finished with value: 0.726048249458001 and parameters: {'n_estimators': 41, 'max_depth': 6, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:28,785] Trial 61 finished with value: 0.7337208050801851 and parameters: {'n_estimators': 33, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:29,089] Trial 62 finished with value: 0.7338386866962908 and parameters: {'n_estimators': 35, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:29,389] Trial 63 finished with value: 0.7325471142067849 and parameters: {'n_estimators': 27, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:29,747] Trial 64 finished with value: 0.729671827831337 and parameters: {'n_estimators': 50, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 49 with value: 0.7353403959797244.\n","[I 2023-09-27 18:55:30,085] Trial 65 finished with value: 0.7381644294786044 and parameters: {'n_estimators': 42, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:30,384] Trial 66 finished with value: 0.7267350380040078 and parameters: {'n_estimators': 41, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:30,795] Trial 67 finished with value: 0.7299844703783999 and parameters: {'n_estimators': 55, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:31,262] Trial 68 finished with value: 0.6961626971313765 and parameters: {'n_estimators': 44, 'max_depth': 9, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:31,625] Trial 69 finished with value: 0.7136655544792452 and parameters: {'n_estimators': 50, 'max_depth': 4, 'criterion': 'gini'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:31,869] Trial 70 finished with value: 0.7265120879909385 and parameters: {'n_estimators': 23, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:32,187] Trial 71 finished with value: 0.7353403959797244 and parameters: {'n_estimators': 36, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:32,568] Trial 72 finished with value: 0.7353403959797244 and parameters: {'n_estimators': 36, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:32,879] Trial 73 finished with value: 0.7353403959797244 and parameters: {'n_estimators': 36, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 65 with value: 0.7381644294786044.\n","[I 2023-09-27 18:55:33,243] Trial 74 finished with value: 0.7396302617484406 and parameters: {'n_estimators': 37, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:33,566] Trial 75 finished with value: 0.7396302617484406 and parameters: {'n_estimators': 37, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:33,902] Trial 76 finished with value: 0.7258278620887597 and parameters: {'n_estimators': 37, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:34,147] Trial 77 finished with value: 0.7286698340944385 and parameters: {'n_estimators': 18, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:34,400] Trial 78 finished with value: 0.7272142523999159 and parameters: {'n_estimators': 25, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:34,694] Trial 79 finished with value: 0.7295078186263204 and parameters: {'n_estimators': 31, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:34,964] Trial 80 finished with value: 0.7190676076694805 and parameters: {'n_estimators': 37, 'max_depth': 2, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:35,268] Trial 81 finished with value: 0.7310966577999191 and parameters: {'n_estimators': 32, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:35,601] Trial 82 finished with value: 0.7382925616700238 and parameters: {'n_estimators': 38, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:35,948] Trial 83 finished with value: 0.7378979145204524 and parameters: {'n_estimators': 39, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:36,262] Trial 84 finished with value: 0.729118296764406 and parameters: {'n_estimators': 26, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:36,636] Trial 85 finished with value: 0.7258791149653274 and parameters: {'n_estimators': 38, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:36,953] Trial 86 finished with value: 0.725735606910938 and parameters: {'n_estimators': 20, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:37,291] Trial 87 finished with value: 0.7103366801461732 and parameters: {'n_estimators': 29, 'max_depth': 4, 'criterion': 'gini'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:37,546] Trial 88 finished with value: 0.715994997719247 and parameters: {'n_estimators': 11, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:37,936] Trial 89 finished with value: 0.7311171589505461 and parameters: {'n_estimators': 48, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:38,245] Trial 90 finished with value: 0.7338386866962908 and parameters: {'n_estimators': 35, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:38,570] Trial 91 finished with value: 0.7378979145204524 and parameters: {'n_estimators': 39, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:38,897] Trial 92 finished with value: 0.7376365248499572 and parameters: {'n_estimators': 40, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:39,227] Trial 93 finished with value: 0.7376365248499572 and parameters: {'n_estimators': 40, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:39,555] Trial 94 finished with value: 0.7378979145204524 and parameters: {'n_estimators': 39, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:39,912] Trial 95 finished with value: 0.7223631676327833 and parameters: {'n_estimators': 52, 'max_depth': 3, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:40,283] Trial 96 finished with value: 0.7336490510529904 and parameters: {'n_estimators': 41, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:40,736] Trial 97 finished with value: 0.7198825284069068 and parameters: {'n_estimators': 40, 'max_depth': 7, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:41,131] Trial 98 finished with value: 0.7311171589505461 and parameters: {'n_estimators': 48, 'max_depth': 4, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:41,788] Trial 99 finished with value: 0.7291234220520626 and parameters: {'n_estimators': 58, 'max_depth': 5, 'criterion': 'entropy'}. Best is trial 74 with value: 0.7396302617484406.\n","[I 2023-09-27 18:55:41,790] A new study created in memory with name: no-name-68903b56-54cb-4c2f-9ef5-702b4d15ae32\n","[I 2023-09-27 18:55:42,147] Trial 0 finished with value: 0.7120767153056465 and parameters: {'learning_rate': 0.12520653814999466, 'num_leaves': 12, 'max_depth': 7, 'min_child_samples': 28, 'subsample': 0.4812893194050143, 'colsample_bytree': 0.6813047017599905, 'reg_alpha': 0.21879360563134626, 'reg_lambda': 0.8917730007820798}. Best is trial 0 with value: 0.7120767153056465.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001825 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:42,479] Trial 1 finished with value: 0.6249058228393068 and parameters: {'learning_rate': 0.8459126528049372, 'num_leaves': 7, 'max_depth': 8, 'min_child_samples': 27, 'subsample': 0.6112401049845391, 'colsample_bytree': 0.933036974463395, 'reg_alpha': 0.03551802909894347, 'reg_lambda': 0.08712929970154071}. Best is trial 0 with value: 0.7120767153056465.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:42,814] Trial 2 finished with value: 0.7292387410243399 and parameters: {'learning_rate': 0.01097581541938016, 'num_leaves': 13, 'max_depth': 8, 'min_child_samples': 44, 'subsample': 0.9807565080094875, 'colsample_bytree': 0.8192427077950513, 'reg_alpha': 0.23073968112646592, 'reg_lambda': 0.7805291762864555}. Best is trial 2 with value: 0.7292387410243399.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:43,203] Trial 3 finished with value: 0.7217096934565452 and parameters: {'learning_rate': 0.017240460099242298, 'num_leaves': 10, 'max_depth': 2, 'min_child_samples': 48, 'subsample': 0.5696634895750645, 'colsample_bytree': 0.4731957459914713, 'reg_alpha': 0.13227780605231348, 'reg_lambda': 0.7742336894342167}. Best is trial 2 with value: 0.7292387410243399.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001185 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:43,474] Trial 4 finished with value: 0.7358196103756323 and parameters: {'learning_rate': 0.08171478915115864, 'num_leaves': 9, 'max_depth': 1, 'min_child_samples': 31, 'subsample': 0.6508861504501793, 'colsample_bytree': 0.6552405971872812, 'reg_alpha': 0.4718740392573121, 'reg_lambda': 0.6818202991034834}. Best is trial 4 with value: 0.7358196103756323.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001432 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:43,798] Trial 5 finished with value: 0.7298819646252646 and parameters: {'learning_rate': 0.05236194873419147, 'num_leaves': 8, 'max_depth': 7, 'min_child_samples': 4, 'subsample': 0.7000900439011009, 'colsample_bytree': 0.7035740826563435, 'reg_alpha': 0.10519128053692045, 'reg_lambda': 0.1289262976548533}. Best is trial 4 with value: 0.7358196103756323.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000860 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3145\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 73\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:44,098] Trial 6 finished with value: 0.7281342415343061 and parameters: {'learning_rate': 0.042742183240368416, 'num_leaves': 7, 'max_depth': 6, 'min_child_samples': 22, 'subsample': 0.9895364542533036, 'colsample_bytree': 0.19184032967322528, 'reg_alpha': 0.10443837804741735, 'reg_lambda': 0.16130951788499626}. Best is trial 4 with value: 0.7358196103756323.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000475 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3127\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 67\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:44,395] Trial 7 finished with value: 0.7126558728108615 and parameters: {'learning_rate': 0.20240286281332814, 'num_leaves': 5, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.24307262528096776, 'colsample_bytree': 0.1993376270478746, 'reg_alpha': 0.3281647947326367, 'reg_lambda': 0.1381829513486138}. Best is trial 4 with value: 0.7358196103756323.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000475 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:44,641] Trial 8 finished with value: 0.7076587173455109 and parameters: {'learning_rate': 0.02472661821977644, 'num_leaves': 7, 'max_depth': 9, 'min_child_samples': 5, 'subsample': 0.8541504167489236, 'colsample_bytree': 0.18648856710456677, 'reg_alpha': 0.4882297325066979, 'reg_lambda': 0.4686512016477016}. Best is trial 4 with value: 0.7358196103756323.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3142\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 72\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:44,924] Trial 9 finished with value: 0.653299916457811 and parameters: {'learning_rate': 0.8985084719406056, 'num_leaves': 10, 'max_depth': 8, 'min_child_samples': 2, 'subsample': 0.3545262663187686, 'colsample_bytree': 0.20817690509185202, 'reg_alpha': 0.14807009876107247, 'reg_lambda': 0.11872771895424405}. Best is trial 4 with value: 0.7358196103756323.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3151\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 75\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:45,191] Trial 10 finished with value: 0.7338745637098882 and parameters: {'learning_rate': 0.24082262250355246, 'num_leaves': 3, 'max_depth': 1, 'min_child_samples': 37, 'subsample': 0.12495042392317168, 'colsample_bytree': 0.5124541926058219, 'reg_alpha': 0.4370159738357042, 'reg_lambda': 0.5475027045393774}. Best is trial 4 with value: 0.7358196103756323.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:45,461] Trial 11 finished with value: 0.73661659260626 and parameters: {'learning_rate': 0.2956779788159376, 'num_leaves': 2, 'max_depth': 1, 'min_child_samples': 38, 'subsample': 0.16087951336819906, 'colsample_bytree': 0.5018364062135304, 'reg_alpha': 0.4569508155164782, 'reg_lambda': 0.5536504924106238}. Best is trial 11 with value: 0.73661659260626.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:45,796] Trial 12 finished with value: 0.7307583888145722 and parameters: {'learning_rate': 0.33083685463862916, 'num_leaves': 2, 'max_depth': 3, 'min_child_samples': 36, 'subsample': 0.3962033138202408, 'colsample_bytree': 0.3966764903509043, 'reg_alpha': 0.3854863739440787, 'reg_lambda': 0.5799377279362194}. Best is trial 11 with value: 0.73661659260626.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000952 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:46,121] Trial 13 finished with value: 0.7291234220520627 and parameters: {'learning_rate': 0.06400802472096613, 'num_leaves': 15, 'max_depth': 4, 'min_child_samples': 35, 'subsample': 0.11624798389459384, 'colsample_bytree': 0.6359851811332385, 'reg_alpha': 0.4918300104768254, 'reg_lambda': 0.4092484370032766}. Best is trial 11 with value: 0.73661659260626.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3117\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 64\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:46,392] Trial 14 finished with value: 0.7374904541517393 and parameters: {'learning_rate': 0.1203460243845841, 'num_leaves': 4, 'max_depth': 1, 'min_child_samples': 19, 'subsample': 0.7003766705696773, 'colsample_bytree': 0.4129876596360761, 'reg_alpha': 0.3515102521646851, 'reg_lambda': 0.6771522270012464}. Best is trial 14 with value: 0.7374904541517393.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000519 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:46,729] Trial 15 finished with value: 0.7352737672401864 and parameters: {'learning_rate': 0.135625812135815, 'num_leaves': 4, 'max_depth': 3, 'min_child_samples': 19, 'subsample': 0.2655308310206551, 'colsample_bytree': 0.4109285966189733, 'reg_alpha': 0.33114286639022966, 'reg_lambda': 0.975819805360121}. Best is trial 14 with value: 0.7374904541517393.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:47,053] Trial 16 finished with value: 0.7074690817022106 and parameters: {'learning_rate': 0.45829082577153624, 'num_leaves': 5, 'max_depth': 2, 'min_child_samples': 14, 'subsample': 0.7376547550528517, 'colsample_bytree': 0.365426072865503, 'reg_alpha': 0.40284314189042203, 'reg_lambda': 0.3489786672799289}. Best is trial 14 with value: 0.7374904541517393.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:47,337] Trial 17 finished with value: 0.7360758747584708 and parameters: {'learning_rate': 0.15168519340239375, 'num_leaves': 2, 'max_depth': 1, 'min_child_samples': 43, 'subsample': 0.4881696052778068, 'colsample_bytree': 0.31188706893715845, 'reg_alpha': 0.3453688668795294, 'reg_lambda': 0.6123361980468363}. Best is trial 14 with value: 0.7374904541517393.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000657 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:47,671] Trial 18 finished with value: 0.7425055481238885 and parameters: {'learning_rate': 0.10514274550769727, 'num_leaves': 5, 'max_depth': 4, 'min_child_samples': 16, 'subsample': 0.7698235231529659, 'colsample_bytree': 0.5601994969837869, 'reg_alpha': 0.2743910510562609, 'reg_lambda': 0.3315204189813712}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001195 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:47,962] Trial 19 finished with value: 0.7414651147295641 and parameters: {'learning_rate': 0.09019018606247581, 'num_leaves': 5, 'max_depth': 4, 'min_child_samples': 11, 'subsample': 0.7508902624482873, 'colsample_bytree': 0.6095872857444264, 'reg_alpha': 0.28146975709835376, 'reg_lambda': 0.34286297381164105}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:48,229] Trial 20 finished with value: 0.7310351543480378 and parameters: {'learning_rate': 0.07871132750113158, 'num_leaves': 6, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.8130358174247988, 'colsample_bytree': 0.5564035669780963, 'reg_alpha': 0.2828437829840133, 'reg_lambda': 0.2591070019669436}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000797 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:48,489] Trial 21 finished with value: 0.7321063394683026 and parameters: {'learning_rate': 0.105610581497712, 'num_leaves': 4, 'max_depth': 4, 'min_child_samples': 19, 'subsample': 0.7601894322161632, 'colsample_bytree': 0.5800109927820514, 'reg_alpha': 0.2873989440018569, 'reg_lambda': 0.31650992410414874}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000818 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:48,765] Trial 22 finished with value: 0.7379517300408486 and parameters: {'learning_rate': 0.039209885818092434, 'num_leaves': 5, 'max_depth': 4, 'min_child_samples': 10, 'subsample': 0.6672880991909423, 'colsample_bytree': 0.5813325666597559, 'reg_alpha': 0.27920312864207536, 'reg_lambda': 0.4436384330422293}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:49,028] Trial 23 finished with value: 0.7383694409848752 and parameters: {'learning_rate': 0.042482393429384244, 'num_leaves': 6, 'max_depth': 4, 'min_child_samples': 8, 'subsample': 0.6397141392551939, 'colsample_bytree': 0.7426357647346324, 'reg_alpha': 0.25407607859101167, 'reg_lambda': 0.4425905584884457}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000599 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:49,288] Trial 24 finished with value: 0.7382259329304857 and parameters: {'learning_rate': 0.06427189711212367, 'num_leaves': 6, 'max_depth': 3, 'min_child_samples': 9, 'subsample': 0.8605609307642843, 'colsample_bytree': 0.753574085860097, 'reg_alpha': 0.21661094790981802, 'reg_lambda': 0.2556469868185993}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:49,553] Trial 25 finished with value: 0.7362603851141145 and parameters: {'learning_rate': 0.03141754650145888, 'num_leaves': 6, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.608818471298584, 'colsample_bytree': 0.7727506694752324, 'reg_alpha': 0.24206479548210988, 'reg_lambda': 0.4016586317149513}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:49,901] Trial 26 finished with value: 0.7351046327475129 and parameters: {'learning_rate': 0.08657091839329419, 'num_leaves': 8, 'max_depth': 5, 'min_child_samples': 23, 'subsample': 0.7743906834197736, 'colsample_bytree': 0.617064043516138, 'reg_alpha': 0.20009047626097243, 'reg_lambda': 0.4897914966548871}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002020 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:50,202] Trial 27 finished with value: 0.7361450661418372 and parameters: {'learning_rate': 0.048305846980417115, 'num_leaves': 3, 'max_depth': 4, 'min_child_samples': 7, 'subsample': 0.5305978772466864, 'colsample_bytree': 0.8527590686384392, 'reg_alpha': 0.2668777907494781, 'reg_lambda': 0.004440639674382685}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000815 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:50,478] Trial 28 finished with value: 0.7119537084018841 and parameters: {'learning_rate': 0.1681459248419481, 'num_leaves': 9, 'max_depth': 3, 'min_child_samples': 1, 'subsample': 0.8969718050618166, 'colsample_bytree': 0.9911533207380818, 'reg_alpha': 0.30323401953490997, 'reg_lambda': 0.3400032540918149}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000645 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3154\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 76\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:50,761] Trial 29 finished with value: 0.7195032571203056 and parameters: {'learning_rate': 0.104830197831262, 'num_leaves': 12, 'max_depth': 6, 'min_child_samples': 16, 'subsample': 0.7302217886049865, 'colsample_bytree': 0.7040073487485305, 'reg_alpha': 0.2520858726089235, 'reg_lambda': 0.24763600053258128}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000797 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:51,067] Trial 30 finished with value: 0.7360015580874474 and parameters: {'learning_rate': 0.032835232636661986, 'num_leaves': 5, 'max_depth': 10, 'min_child_samples': 12, 'subsample': 0.79723272012123, 'colsample_bytree': 0.673230871665655, 'reg_alpha': 0.20136749503493484, 'reg_lambda': 0.38525281043987814}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:51,328] Trial 31 finished with value: 0.7367139730717387 and parameters: {'learning_rate': 0.06446524901162287, 'num_leaves': 6, 'max_depth': 3, 'min_child_samples': 8, 'subsample': 0.8915975890897591, 'colsample_bytree': 0.7640683693184608, 'reg_alpha': 0.2029491710632656, 'reg_lambda': 0.26300215872492205}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001446 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:51,586] Trial 32 finished with value: 0.7414728026610493 and parameters: {'learning_rate': 0.06263808121469797, 'num_leaves': 6, 'max_depth': 2, 'min_child_samples': 7, 'subsample': 0.639026188843008, 'colsample_bytree': 0.7206125218827768, 'reg_alpha': 0.22625800942716673, 'reg_lambda': 0.19928521817794095}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:51,870] Trial 33 finished with value: 0.733080144123089 and parameters: {'learning_rate': 0.10957324741572906, 'num_leaves': 7, 'max_depth': 2, 'min_child_samples': 5, 'subsample': 0.6402118921469364, 'colsample_bytree': 0.6070074950691123, 'reg_alpha': 0.2458692735404556, 'reg_lambda': 0.18904000145276972}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001385 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3142\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 72\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:52,146] Trial 34 finished with value: 0.7357837333620348 and parameters: {'learning_rate': 0.06513493937446217, 'num_leaves': 3, 'max_depth': 4, 'min_child_samples': 16, 'subsample': 0.6030592607219939, 'colsample_bytree': 0.8527138745832988, 'reg_alpha': 0.30699038152886876, 'reg_lambda': 0.32063180010204634}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000671 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:52,403] Trial 35 finished with value: 0.7363859546617054 and parameters: {'learning_rate': 0.049919367830674403, 'num_leaves': 7, 'max_depth': 2, 'min_child_samples': 27, 'subsample': 0.578619740642659, 'colsample_bytree': 0.7040130767959024, 'reg_alpha': 0.17238901736685047, 'reg_lambda': 0.2149519809395348}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001492 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:52,671] Trial 36 finished with value: 0.7383028122453372 and parameters: {'learning_rate': 0.08594743369978942, 'num_leaves': 4, 'max_depth': 5, 'min_child_samples': 11, 'subsample': 0.6778027417893381, 'colsample_bytree': 0.5398034378207617, 'reg_alpha': 0.23488532375305066, 'reg_lambda': 0.45647532523829915}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:52,946] Trial 37 finished with value: 0.7325009866178739 and parameters: {'learning_rate': 0.02129701741899755, 'num_leaves': 8, 'max_depth': 7, 'min_child_samples': 6, 'subsample': 0.649092791878623, 'colsample_bytree': 0.6429590551391561, 'reg_alpha': 0.25788757002129414, 'reg_lambda': 0.30518063988567756}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001300 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3139\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 71\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:53,194] Trial 38 finished with value: 0.7232959699863156 and parameters: {'learning_rate': 0.013027194251382997, 'num_leaves': 10, 'max_depth': 2, 'min_child_samples': 2, 'subsample': 0.7277819369399429, 'colsample_bytree': 0.6796229254038267, 'reg_alpha': 0.3035158871136999, 'reg_lambda': 0.19485453162500196}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3151\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 75\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:53,478] Trial 39 finished with value: 0.7352763298840148 and parameters: {'learning_rate': 0.03899035329889442, 'num_leaves': 6, 'max_depth': 5, 'min_child_samples': 23, 'subsample': 0.5626530838865935, 'colsample_bytree': 0.473433195948866, 'reg_alpha': 0.22177093092963995, 'reg_lambda': 0.3733002037734749}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000509 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:53,739] Trial 40 finished with value: 0.7292361783805116 and parameters: {'learning_rate': 0.14428489987999568, 'num_leaves': 5, 'max_depth': 6, 'min_child_samples': 17, 'subsample': 0.6904120832350074, 'colsample_bytree': 0.730232894466982, 'reg_alpha': 0.1683522682056658, 'reg_lambda': 0.41805239785383663}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:54,008] Trial 41 finished with value: 0.7359554304985368 and parameters: {'learning_rate': 0.0846109652019311, 'num_leaves': 4, 'max_depth': 5, 'min_child_samples': 11, 'subsample': 0.6610773420723687, 'colsample_bytree': 0.5518504451462152, 'reg_alpha': 0.23248086470290275, 'reg_lambda': 0.47095256041069994}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:54,280] Trial 42 finished with value: 0.7351763867747078 and parameters: {'learning_rate': 0.09025655113127468, 'num_leaves': 4, 'max_depth': 4, 'min_child_samples': 12, 'subsample': 0.7068875358747733, 'colsample_bytree': 0.6166508522538918, 'reg_alpha': 0.26305367199409024, 'reg_lambda': 0.5151786318319272}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000835 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:54,585] Trial 43 finished with value: 0.7331467728626269 and parameters: {'learning_rate': 0.05766401359596361, 'num_leaves': 3, 'max_depth': 3, 'min_child_samples': 7, 'subsample': 0.6308779554498452, 'colsample_bytree': 0.5308878823205059, 'reg_alpha': 0.2699051893512414, 'reg_lambda': 0.4324336343643367}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:54,899] Trial 44 finished with value: 0.7206154445418249 and parameters: {'learning_rate': 0.12044018341774328, 'num_leaves': 7, 'max_depth': 5, 'min_child_samples': 4, 'subsample': 0.767815923168105, 'colsample_bytree': 0.6643836000621619, 'reg_alpha': 0.01996882127917532, 'reg_lambda': 0.3687138622647176}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3145\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 73\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:55,226] Trial 45 finished with value: 0.7395943847348432 and parameters: {'learning_rate': 0.08046286675915296, 'num_leaves': 5, 'max_depth': 7, 'min_child_samples': 31, 'subsample': 0.6806910569527366, 'colsample_bytree': 0.5808031510808425, 'reg_alpha': 0.23231450094271355, 'reg_lambda': 0.5007518072882172}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000557 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:55,557] Trial 46 finished with value: 0.7381695547662612 and parameters: {'learning_rate': 0.051887356843732656, 'num_leaves': 5, 'max_depth': 8, 'min_child_samples': 30, 'subsample': 0.5917078517882258, 'colsample_bytree': 0.5882894313804696, 'reg_alpha': 0.18688530279900906, 'reg_lambda': 0.517458285181135}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000661 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:55,886] Trial 47 finished with value: 0.7187703409853876 and parameters: {'learning_rate': 0.17219769711621039, 'num_leaves': 8, 'max_depth': 7, 'min_child_samples': 29, 'subsample': 0.5431951176144971, 'colsample_bytree': 0.6575038836909246, 'reg_alpha': 0.22125453683337976, 'reg_lambda': 0.3056544548290474}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001585 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:56,201] Trial 48 finished with value: 0.7355710339242789 and parameters: {'learning_rate': 0.0761388603505238, 'num_leaves': 6, 'max_depth': 7, 'min_child_samples': 31, 'subsample': 0.6272093993770178, 'colsample_bytree': 0.8101965709277342, 'reg_alpha': 0.1397497430528561, 'reg_lambda': 0.1589321837198279}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:56,544] Trial 49 finished with value: 0.7380696116569541 and parameters: {'learning_rate': 0.04616952444539229, 'num_leaves': 5, 'max_depth': 8, 'min_child_samples': 32, 'subsample': 0.7039325711483654, 'colsample_bytree': 0.5014736796986597, 'reg_alpha': 0.24708169213574532, 'reg_lambda': 0.4139791066978416}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:56,846] Trial 50 finished with value: 0.7337618073814391 and parameters: {'learning_rate': 0.12362572016449573, 'num_leaves': 3, 'max_depth': 4, 'min_child_samples': 26, 'subsample': 0.817482381767189, 'colsample_bytree': 0.7195332835349261, 'reg_alpha': 0.287413113115225, 'reg_lambda': 0.10501476266403095}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001355 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:57,162] Trial 51 finished with value: 0.7331980257391947 and parameters: {'learning_rate': 0.09378449477974062, 'num_leaves': 4, 'max_depth': 6, 'min_child_samples': 13, 'subsample': 0.6902106424668509, 'colsample_bytree': 0.5564313083785882, 'reg_alpha': 0.2347775814475529, 'reg_lambda': 0.4598825828780277}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000765 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:57,484] Trial 52 finished with value: 0.737441763919 and parameters: {'learning_rate': 0.07755122982922026, 'num_leaves': 4, 'max_depth': 9, 'min_child_samples': 34, 'subsample': 0.6690667665647282, 'colsample_bytree': 0.6345225156927607, 'reg_alpha': 0.2222468551416238, 'reg_lambda': 0.49809887112278756}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000954 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:57,826] Trial 53 finished with value: 0.7374110121930592 and parameters: {'learning_rate': 0.06726827020646205, 'num_leaves': 5, 'max_depth': 5, 'min_child_samples': 39, 'subsample': 0.7489043682763395, 'colsample_bytree': 0.46814127442467557, 'reg_alpha': 0.31607342824917606, 'reg_lambda': 0.36254860157292573}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000527 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:58,160] Trial 54 finished with value: 0.7319987084275105 and parameters: {'learning_rate': 0.09990788789955565, 'num_leaves': 6, 'max_depth': 4, 'min_child_samples': 4, 'subsample': 0.6200212725818756, 'colsample_bytree': 0.5947789332847921, 'reg_alpha': 0.271096663180601, 'reg_lambda': 0.4472118431045382}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001573 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3145\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 73\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:58,493] Trial 55 finished with value: 0.7358375488824309 and parameters: {'learning_rate': 0.05573633964947156, 'num_leaves': 7, 'max_depth': 3, 'min_child_samples': 10, 'subsample': 0.730902775787863, 'colsample_bytree': 0.5366316158266156, 'reg_alpha': 0.2893596318032472, 'reg_lambda': 0.5839978973866884}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:58,780] Trial 56 finished with value: 0.7345331631737831 and parameters: {'learning_rate': 0.07535053953373859, 'num_leaves': 2, 'max_depth': 7, 'min_child_samples': 21, 'subsample': 0.6710524410541209, 'colsample_bytree': 0.6936296069124771, 'reg_alpha': 0.25125401877109005, 'reg_lambda': 0.5450526348024073}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001462 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:59,123] Trial 57 finished with value: 0.730071600268565 and parameters: {'learning_rate': 0.13410035770336756, 'num_leaves': 4, 'max_depth': 5, 'min_child_samples': 8, 'subsample': 0.7852573995983774, 'colsample_bytree': 0.6247285282330227, 'reg_alpha': 0.32935534771364183, 'reg_lambda': 0.39986636803427067}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000575 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:59,493] Trial 58 finished with value: 0.7297435818585318 and parameters: {'learning_rate': 0.04280921692134484, 'num_leaves': 15, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.5347306125204714, 'colsample_bytree': 0.5792391264837963, 'reg_alpha': 0.23360599185507158, 'reg_lambda': 0.4805803044842408}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:55:59,820] Trial 59 finished with value: 0.7334440395467196 and parameters: {'learning_rate': 0.09613986240508511, 'num_leaves': 9, 'max_depth': 3, 'min_child_samples': 17, 'subsample': 0.5832907170108532, 'colsample_bytree': 0.6647544053686318, 'reg_alpha': 0.3574347278900937, 'reg_lambda': 0.29310425333804324}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001744 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:00,200] Trial 60 finished with value: 0.7381951812045451 and parameters: {'learning_rate': 0.07157252116914735, 'num_leaves': 6, 'max_depth': 4, 'min_child_samples': 9, 'subsample': 0.7607071277550645, 'colsample_bytree': 0.7423209816113038, 'reg_alpha': 0.20465254170666128, 'reg_lambda': 0.3435613295435971}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:00,583] Trial 61 finished with value: 0.7364115810999893 and parameters: {'learning_rate': 0.0846292172017337, 'num_leaves': 5, 'max_depth': 3, 'min_child_samples': 9, 'subsample': 0.8232994532499393, 'colsample_bytree': 0.737914233452221, 'reg_alpha': 0.22165546198982938, 'reg_lambda': 0.2742742981976555}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001264 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:00,895] Trial 62 finished with value: 0.7352814551716715 and parameters: {'learning_rate': 0.05999960468796026, 'num_leaves': 6, 'max_depth': 2, 'min_child_samples': 6, 'subsample': 0.8352455650468464, 'colsample_bytree': 0.7829735732248148, 'reg_alpha': 0.2629816079916212, 'reg_lambda': 0.2569875002228954}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000610 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3139\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 71\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:01,220] Trial 63 finished with value: 0.7375852719733895 and parameters: {'learning_rate': 0.057560595814396105, 'num_leaves': 5, 'max_depth': 4, 'min_child_samples': 11, 'subsample': 0.8547166778961645, 'colsample_bytree': 0.6856648560143652, 'reg_alpha': 0.20867704462129133, 'reg_lambda': 0.22877325646209268}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:01,515] Trial 64 finished with value: 0.723552234369154 and parameters: {'learning_rate': 0.1141598911573177, 'num_leaves': 7, 'max_depth': 3, 'min_child_samples': 3, 'subsample': 0.7947938884210153, 'colsample_bytree': 0.6377731554061635, 'reg_alpha': 0.2421749205717328, 'reg_lambda': 0.3269916711452689}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3145\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 73\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:01,810] Trial 65 finished with value: 0.7362193828128603 and parameters: {'learning_rate': 0.0700700700389534, 'num_leaves': 3, 'max_depth': 2, 'min_child_samples': 13, 'subsample': 0.7116300587946114, 'colsample_bytree': 0.7574619359090828, 'reg_alpha': 0.1864427992606325, 'reg_lambda': 0.2893788728924171}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:02,131] Trial 66 finished with value: 0.7373546340288349 and parameters: {'learning_rate': 0.10626695828672963, 'num_leaves': 6, 'max_depth': 4, 'min_child_samples': 8, 'subsample': 0.7414006917092689, 'colsample_bytree': 0.7105636814675842, 'reg_alpha': 0.283678496901214, 'reg_lambda': 0.37705371400688953}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:02,404] Trial 67 finished with value: 0.7335747343819672 and parameters: {'learning_rate': 0.06102840908068929, 'num_leaves': 4, 'max_depth': 1, 'min_child_samples': 20, 'subsample': 0.6442762836821285, 'colsample_bytree': 0.7870740408604088, 'reg_alpha': 0.25729319473043444, 'reg_lambda': 0.4422249486509224}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:02,713] Trial 68 finished with value: 0.7391561726401894 and parameters: {'learning_rate': 0.03836446562331594, 'num_leaves': 6, 'max_depth': 5, 'min_child_samples': 15, 'subsample': 0.6818264012701156, 'colsample_bytree': 0.5742534290856682, 'reg_alpha': 0.21313855500288387, 'reg_lambda': 0.22532753590849564}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001010 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:03,004] Trial 69 finished with value: 0.7385155116830933 and parameters: {'learning_rate': 0.0358897539986094, 'num_leaves': 7, 'max_depth': 5, 'min_child_samples': 18, 'subsample': 0.68051311850868, 'colsample_bytree': 0.5750108164686968, 'reg_alpha': 0.23704842043355687, 'reg_lambda': 0.21372802217881806}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:03,284] Trial 70 finished with value: 0.7367011598525968 and parameters: {'learning_rate': 0.04112029373965329, 'num_leaves': 7, 'max_depth': 6, 'min_child_samples': 23, 'subsample': 0.6072592695879556, 'colsample_bytree': 0.5783969555458756, 'reg_alpha': 0.29487056260380884, 'reg_lambda': 0.1406379059413153}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:03,625] Trial 71 finished with value: 0.7361988816622334 and parameters: {'learning_rate': 0.034266193171601816, 'num_leaves': 8, 'max_depth': 5, 'min_child_samples': 18, 'subsample': 0.677630726635009, 'colsample_bytree': 0.5137541314367429, 'reg_alpha': 0.27298734247296536, 'reg_lambda': 0.18977629569597854}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:03,956] Trial 72 finished with value: 0.7413190440313463 and parameters: {'learning_rate': 0.049386853784787084, 'num_leaves': 5, 'max_depth': 5, 'min_child_samples': 12, 'subsample': 0.7208466304828067, 'colsample_bytree': 0.5674969805316645, 'reg_alpha': 0.2369485630248659, 'reg_lambda': 0.22078852561793244}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:04,319] Trial 73 finished with value: 0.7376032104801881 and parameters: {'learning_rate': 0.027644371822917933, 'num_leaves': 5, 'max_depth': 5, 'min_child_samples': 15, 'subsample': 0.6500231168737414, 'colsample_bytree': 0.5626677572171441, 'reg_alpha': 0.24667222109553183, 'reg_lambda': 0.22340915247265408}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:04,693] Trial 74 finished with value: 0.7392971180507506 and parameters: {'learning_rate': 0.049860395443301356, 'num_leaves': 6, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.7264562870535551, 'colsample_bytree': 0.6120421560412074, 'reg_alpha': 0.21189644693879936, 'reg_lambda': 0.2345068004256611}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000829 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:05,055] Trial 75 finished with value: 0.7414343630036235 and parameters: {'learning_rate': 0.050474346946749286, 'num_leaves': 6, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.719674256660591, 'colsample_bytree': 0.6159481357264122, 'reg_alpha': 0.191486238238204, 'reg_lambda': 0.2369440432325036}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001751 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:05,441] Trial 76 finished with value: 0.73869489675108 and parameters: {'learning_rate': 0.049700498955203995, 'num_leaves': 5, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.7207488420791922, 'colsample_bytree': 0.6161774347524397, 'reg_alpha': 0.18762301193573655, 'reg_lambda': 0.1729915061491633}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001749 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:05,734] Trial 77 finished with value: 0.7382669352317399 and parameters: {'learning_rate': 0.045346853618447876, 'num_leaves': 6, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.7571050651778771, 'colsample_bytree': 0.5966243807207394, 'reg_alpha': 0.15922463372334053, 'reg_lambda': 0.2729931186449662}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:06,104] Trial 78 finished with value: 0.729851212899324 and parameters: {'learning_rate': 0.053675765954394924, 'num_leaves': 12, 'max_depth': 7, 'min_child_samples': 49, 'subsample': 0.7159844846024007, 'colsample_bytree': 0.6093823497080665, 'reg_alpha': 0.21113810512746434, 'reg_lambda': 0.23213772377410674}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:06,416] Trial 79 finished with value: 0.7305431267329878 and parameters: {'learning_rate': 0.03858857207211241, 'num_leaves': 13, 'max_depth': 6, 'min_child_samples': 12, 'subsample': 0.7848143490826022, 'colsample_bytree': 0.6473868227075967, 'reg_alpha': 0.19176279364182677, 'reg_lambda': 0.14794181929953093}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001589 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:06,708] Trial 80 finished with value: 0.7342077074075782 and parameters: {'learning_rate': 0.047381481794362475, 'num_leaves': 6, 'max_depth': 7, 'min_child_samples': 25, 'subsample': 0.7450195641699598, 'colsample_bytree': 0.6017229495317418, 'reg_alpha': 0.1686841855927256, 'reg_lambda': 0.17962366570351518}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000965 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:06,964] Trial 81 finished with value: 0.7403247382259328 and parameters: {'learning_rate': 0.053155959504299743, 'num_leaves': 5, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.7159982175246875, 'colsample_bytree': 0.6213133626448912, 'reg_alpha': 0.19422541450827027, 'reg_lambda': 0.16717783881928938}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:07,304] Trial 82 finished with value: 0.7404784968556362 and parameters: {'learning_rate': 0.05351319230882574, 'num_leaves': 5, 'max_depth': 7, 'min_child_samples': 16, 'subsample': 0.6968082690903687, 'colsample_bytree': 0.5599463093277517, 'reg_alpha': 0.21915438881756877, 'reg_lambda': 0.10914449491648184}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001137 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:07,582] Trial 83 finished with value: 0.7383130628206507 and parameters: {'learning_rate': 0.06414907456682264, 'num_leaves': 5, 'max_depth': 7, 'min_child_samples': 16, 'subsample': 0.7698855122937189, 'colsample_bytree': 0.6493363564516511, 'reg_alpha': 0.22708806513595264, 'reg_lambda': 0.11895851421766065}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001656 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:08,006] Trial 84 finished with value: 0.7373315702343795 and parameters: {'learning_rate': 0.05473635367708994, 'num_leaves': 5, 'max_depth': 8, 'min_child_samples': 18, 'subsample': 0.7065805957972304, 'colsample_bytree': 0.55173553247145, 'reg_alpha': 0.20048857603264947, 'reg_lambda': 0.0893176892672932}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001125 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:08,352] Trial 85 finished with value: 0.7377390306030925 and parameters: {'learning_rate': 0.07466656757077815, 'num_leaves': 4, 'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.7955820219429073, 'colsample_bytree': 0.6277867294840875, 'reg_alpha': 0.1768260129074851, 'reg_lambda': 0.16671730554011377}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001568 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:08,716] Trial 86 finished with value: 0.7388665938875819 and parameters: {'learning_rate': 0.05039040075912975, 'num_leaves': 5, 'max_depth': 6, 'min_child_samples': 13, 'subsample': 0.746270956016496, 'colsample_bytree': 0.5238587560185238, 'reg_alpha': 0.12322999746841118, 'reg_lambda': 0.04420949911519029}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:08,984] Trial 87 finished with value: 0.7373187570152375 and parameters: {'learning_rate': 0.06720412818729841, 'num_leaves': 4, 'max_depth': 7, 'min_child_samples': 10, 'subsample': 0.7267325203590022, 'colsample_bytree': 0.49849870762653087, 'reg_alpha': 0.2151009029712725, 'reg_lambda': 0.20057233671008728}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3136\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 70\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:09,307] Trial 88 finished with value: 0.7378722880821686 and parameters: {'learning_rate': 0.08018969327557667, 'num_leaves': 5, 'max_depth': 6, 'min_child_samples': 12, 'subsample': 0.704617675202078, 'colsample_bytree': 0.6832258849550383, 'reg_alpha': 0.1966976910287757, 'reg_lambda': 0.1345660618754868}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001487 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:09,689] Trial 89 finished with value: 0.7350175028573479 and parameters: {'learning_rate': 0.09384007041704025, 'num_leaves': 6, 'max_depth': 9, 'min_child_samples': 17, 'subsample': 0.6462950217031443, 'colsample_bytree': 0.5429644288341888, 'reg_alpha': 0.15610216802470125, 'reg_lambda': 0.2417444145465026}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:09,989] Trial 90 finished with value: 0.7339821947506804 and parameters: {'learning_rate': 0.043310261618715915, 'num_leaves': 3, 'max_depth': 8, 'min_child_samples': 40, 'subsample': 0.6944870417678213, 'colsample_bytree': 0.6628553339392975, 'reg_alpha': 0.18048796163084982, 'reg_lambda': 0.15575058719105694}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001564 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:10,360] Trial 91 finished with value: 0.7374776409325973 and parameters: {'learning_rate': 0.03789451852651974, 'num_leaves': 6, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.6627078343000571, 'colsample_bytree': 0.5987613892675765, 'reg_alpha': 0.21109292051718054, 'reg_lambda': 0.21110236652655212}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000996 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:10,666] Trial 92 finished with value: 0.7388563433122683 and parameters: {'learning_rate': 0.03021500476208923, 'num_leaves': 7, 'max_depth': 5, 'min_child_samples': 15, 'subsample': 0.6877306899835441, 'colsample_bytree': 0.5698069409914358, 'reg_alpha': 0.22648529393540737, 'reg_lambda': 0.24506289671798742}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:10,953] Trial 93 finished with value: 0.7372931305769537 and parameters: {'learning_rate': 0.05676694992189821, 'num_leaves': 5, 'max_depth': 6, 'min_child_samples': 21, 'subsample': 0.6252547040653301, 'colsample_bytree': 0.5678524734791373, 'reg_alpha': 0.2561565585685194, 'reg_lambda': 0.19157579506592826}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3130\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 68\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:11,225] Trial 94 finished with value: 0.7421390900564294 and parameters: {'learning_rate': 0.04541123671058027, 'num_leaves': 6, 'max_depth': 5, 'min_child_samples': 16, 'subsample': 0.7744952972326846, 'colsample_bytree': 0.6215091452053442, 'reg_alpha': 0.19547456106780092, 'reg_lambda': 0.2792220061673614}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001703 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:11,503] Trial 95 finished with value: 0.7389614117092322 and parameters: {'learning_rate': 0.04708699706714274, 'num_leaves': 4, 'max_depth': 7, 'min_child_samples': 11, 'subsample': 0.7616664846863443, 'colsample_bytree': 0.636772772664797, 'reg_alpha': 0.19265359307474225, 'reg_lambda': 0.278741019344024}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000943 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3133\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 69\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:11,768] Trial 96 finished with value: 0.7354480270205165 and parameters: {'learning_rate': 0.06908660029108778, 'num_leaves': 6, 'max_depth': 6, 'min_child_samples': 6, 'subsample': 0.7787743869529888, 'colsample_bytree': 0.613309284615453, 'reg_alpha': 0.27511537311313317, 'reg_lambda': 0.3390965649561087}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000877 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3139\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 71\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:12,054] Trial 97 finished with value: 0.740524624444547 and parameters: {'learning_rate': 0.05968087162567407, 'num_leaves': 4, 'max_depth': 4, 'min_child_samples': 34, 'subsample': 0.7506323241802937, 'colsample_bytree': 0.5393274417109001, 'reg_alpha': 0.2412055608962247, 'reg_lambda': 0.31139070702336846}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000815 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:12,492] Trial 98 finished with value: 0.7368933581397257 and parameters: {'learning_rate': 0.08315670928771703, 'num_leaves': 4, 'max_depth': 4, 'min_child_samples': 32, 'subsample': 0.8139519027815787, 'colsample_bytree': 0.5345661129792113, 'reg_alpha': 0.24076130364399467, 'reg_lambda': 0.3000678628293135}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3121\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 65\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-09-27 18:56:12,828] Trial 99 finished with value: 0.7387640881344466 and parameters: {'learning_rate': 0.05976475733878537, 'num_leaves': 5, 'max_depth': 5, 'min_child_samples': 45, 'subsample': 0.7370298386760102, 'colsample_bytree': 0.5888946602243817, 'reg_alpha': 0.2648582222281249, 'reg_lambda': 0.32408093121023573}. Best is trial 18 with value: 0.7425055481238885.\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 532, number of negative: 5868\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 3113\n","[LightGBM] [Info] Number of data points in the train set: 6400, number of used features: 63\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.083125 -> initscore=-2.400626\n","[LightGBM] [Info] Start training from score -2.400626\n"]}],"source":["study_logistic = optimize_model(LogisticRegression, param_spaces[\"LogisticRegression\"], X_train, y_train, X_val , y_val, n_trials=100)\n","study_rf = optimize_model(RandomForestClassifier, param_spaces[\"RandomForestClassifier\"], X_train, y_train, X_val , y_val, n_trials=100)\n","study_lgbm = optimize_model(LGBMClassifier, param_spaces[\"LGBMClassifier\"], X_train, y_train, X_val , y_val, n_trials=100)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Score for Logistic Regression:  0.7433050929983446\n","Score for Random Forest:  0.7396302617484406\n","Score for LightGBM:  0.7425055481238885\n"]}],"source":["print(\"Score for Logistic Regression: \", study_logistic.best_value)\n","print(\"Score for Random Forest: \", study_rf.best_value)\n","print(\"Score for LightGBM: \", study_lgbm.best_value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVj_gguE5Iy7"},"outputs":[],"source":["def get_metrics(name_model_dict, X, y, threshold = 0.5):\n","    models_dict = {}\n","    for name, model in name_model_dict.items():\n","        if type(model) == list:\n","            y_prob = model[0].predict_proba(X)[:,1]\n","            threshold_model = model[1]\n","            y_pred = (y_prob >= threshold_model).astype('int')\n","        else:\n","            y_prob = model.predict_proba(X)[:,1]\n","            y_pred = (y_prob >= threshold).astype('int')\n","\n","        models_dict[name] = (y_pred, y_prob)\n","\n","    def get_metrics_df(models_dict, y_true,):\n","        metrics_dict = {\n","            \"Overall AUC\": (\n","                lambda x: roc_auc_score(y_true, x), False),\n","            \" ------\": (lambda x: \"\", True),\n","            \"Balanced Accuracy\": (\n","                lambda x: balanced_accuracy_score(y_true, x), True),\n","            \"Accuracy\": (\n","                lambda x: accuracy_score(y_true, x), True),\n","            \"Precision\": (\n","                lambda x: precision_score(y_true, x, zero_division=0), True),\n","            \"Recall\": (\n","                lambda x: recall_score(y_true, x), True),\n","            \"F1\": (\n","                lambda x: f1_score(y_true, x), True),\n","        }\n","        df_dict = {}\n","        for metric_name, (metric_func, use_preds) in metrics_dict.items():\n","            df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores)\n","                                    for model_name, (preds, scores) in models_dict.items()]\n","        return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())\n","\n","    return get_metrics_df(models_dict, y)"]},{"cell_type":"markdown","metadata":{"id":"eb5LLRRksbLS"},"source":["**TODO**\n","\n","*   Add easy way to include default parameters in optuna\n","*   Improve create_pipeline function\n","*   Integrate pipeline package\n","*   Search hyperparameters range for classifiers\n","*   Warnings on pipeline given numeric unique number of values\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ZMaWNR4MI5pyjbCLwUSkh7B_HshPUop8","timestamp":1694628479812}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":0}
