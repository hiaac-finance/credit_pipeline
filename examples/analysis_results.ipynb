{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 13:40:43.257307: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-12 13:40:43.292885: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-12 13:40:43.292905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-12 13:40:43.293920: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-12 13:40:43.299556: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-12 13:40:43.300975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 13:40:44.347355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_hex\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "import joblib\n",
    "from glob import glob\n",
    "import sys\n",
    "from credit_pipeline import training, evaluate\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "from experiments import load_split, PROTECTED_ATTRIBUTES\n",
    "\n",
    "# small fix to be able to load models\n",
    "from credit_pipeline.training import EBE\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_table(metrics):\n",
    "    metrics_mean = metrics.groupby(\"model\").apply(lambda x : x.abs().mean())\n",
    "    metrics_std = metrics.groupby(\"model\").apply(lambda x : x.abs().std())\n",
    "\n",
    "    for col in metrics.columns:\n",
    "        if \"diff\" in col:\n",
    "            metrics_mean[col] = metrics.groupby(\"model\")[col].mean()\n",
    "            metrics_std[col] = metrics.groupby(\"model\")[col].std()\n",
    "\n",
    "    for col in metrics_mean.columns:\n",
    "        metrics_mean[col] = (\n",
    "            \" & \" \n",
    "            + metrics_mean[col].round(3).astype(str)\n",
    "            + \" ± \"\n",
    "            + metrics_std[col].round(3).astype(str)\n",
    "        )\n",
    "    metrics_mean = metrics_mean.reset_index()\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small config to print the tables\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 0)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_credit_experiment_perf(dataset_name, seed = 0):\n",
    "    \"\"\"Function that summarizes the results of the credit models experiment.\n",
    "    It will print the mean and standard deviation of metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    path = \"../results/credit_models_unaware\"\n",
    "    metrics_folds_val = []\n",
    "    metrics_folds_test = []\n",
    "    for fold in range(10):\n",
    "        model_dict = {}\n",
    "        X_train, A_train, Y_train, X_val, A_val, Y_val, X_test, A_test, Y_test = load_split(\n",
    "            dataset_name, fold, seed, unaware = True\n",
    "        )\n",
    "\n",
    "\n",
    "        models_files = glob(f\"{path}/{dataset_name}/{fold}/*.pkl\")\n",
    "        # remove the ones that are not models\n",
    "        models_files = [file for file in models_files if \"study\" not in file]\n",
    "\n",
    "        for file in models_files:\n",
    "            model = joblib.load(file)\n",
    "            model_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            Y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "            threshold = training.ks_threshold(Y_train, Y_train_pred)\n",
    "            model_dict[model_name] = [model, threshold]\n",
    "\n",
    "        metrics_folds_val.append(evaluate.get_metrics(model_dict, X_val, Y_val))\n",
    "        metrics_folds_test.append(evaluate.get_metrics(model_dict, X_test, Y_test))\n",
    "\n",
    "    metrics_val = pd.concat(metrics_folds_val)\n",
    "    metrics_test = pd.concat(metrics_folds_test)\n",
    "\n",
    "    return metrics_val, metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {}\n",
    "for dataset in [\"german\", \"taiwan\", \"homecredit\"]:\n",
    "    _, t = summarize_credit_experiment_perf(dataset, 0)\n",
    "    test_metrics[dataset] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german\n",
      "                    model            AUC    Brier Score Balanced Accuracy  \\\n",
      "0          LGBMClassifier    0.71 ± 0.02  0.225 ± 0.027     0.656 ± 0.033   \n",
      "1      LogisticRegression  0.761 ± 0.008  0.195 ± 0.004     0.701 ± 0.014   \n",
      "2           MLPClassifier   0.755 ± 0.03    0.2 ± 0.015     0.696 ± 0.033   \n",
      "3  RandomForestClassifier  0.748 ± 0.017  0.208 ± 0.011     0.688 ± 0.025   \n",
      "\n",
      "        Accuracy      Precision         Recall             F1  \n",
      "0  0.694 ± 0.022  0.596 ± 0.061  0.521 ± 0.157  0.536 ± 0.079  \n",
      "1   0.71 ± 0.032   0.59 ± 0.053  0.669 ± 0.089   0.621 ± 0.02  \n",
      "2  0.715 ± 0.026   0.595 ± 0.04  0.634 ± 0.099  0.609 ± 0.048  \n",
      "3  0.704 ± 0.028  0.582 ± 0.055  0.634 ± 0.079  0.602 ± 0.032  \n",
      "\n",
      "\n",
      "\n",
      "taiwan\n",
      "                    model            AUC    Brier Score Balanced Accuracy  \\\n",
      "0          LGBMClassifier  0.793 ± 0.001   0.15 ± 0.021     0.721 ± 0.004   \n",
      "1      LogisticRegression   0.77 ± 0.001  0.161 ± 0.025     0.709 ± 0.001   \n",
      "2           MLPClassifier  0.782 ± 0.006  0.135 ± 0.001     0.711 ± 0.004   \n",
      "3  RandomForestClassifier  0.792 ± 0.003  0.158 ± 0.022      0.72 ± 0.003   \n",
      "\n",
      "        Accuracy      Precision         Recall             F1  \n",
      "0    0.74 ± 0.02  0.449 ± 0.025  0.687 ± 0.026   0.542 ± 0.01  \n",
      "1   0.77 ± 0.004  0.488 ± 0.007  0.598 ± 0.007  0.538 ± 0.002  \n",
      "2  0.763 ± 0.011  0.476 ± 0.016  0.619 ± 0.026  0.538 ± 0.004  \n",
      "3  0.731 ± 0.025  0.439 ± 0.031    0.7 ± 0.039   0.538 ± 0.01  \n",
      "\n",
      "\n",
      "\n",
      "homecredit\n",
      "                    model            AUC    Brier Score Balanced Accuracy  \\\n",
      "0          LGBMClassifier  0.753 ± 0.001   0.13 ± 0.066     0.687 ± 0.001   \n",
      "1      LogisticRegression    0.732 ± 0.0    0.208 ± 0.0     0.671 ± 0.001   \n",
      "2           MLPClassifier  0.736 ± 0.001    0.069 ± 0.0     0.674 ± 0.001   \n",
      "3  RandomForestClassifier  0.742 ± 0.002  0.093 ± 0.051     0.677 ± 0.001   \n",
      "\n",
      "        Accuracy      Precision         Recall             F1  \n",
      "0  0.699 ± 0.014  0.165 ± 0.004  0.673 ± 0.017  0.264 ± 0.004  \n",
      "1  0.681 ± 0.013  0.154 ± 0.003   0.66 ± 0.017   0.25 ± 0.003  \n",
      "2  0.682 ± 0.009  0.155 ± 0.002  0.665 ± 0.012  0.252 ± 0.002  \n",
      "3  0.677 ± 0.015  0.155 ± 0.004  0.677 ± 0.018  0.252 ± 0.004  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in test_metrics:\n",
    "    print(dataset)\n",
    "    print(transform_to_table(test_metrics[dataset]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness of Credit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_fair_baselines(dataset_name, seed = 0):\n",
    "    \n",
    "    # computing fairness metrics\n",
    "    path_1 = \"../results/credit_models_unaware\"\n",
    "    path_2 = \"../results/credit_models\"\n",
    "\n",
    "    metrics_folds_val = []\n",
    "    metrics_folds_test = []\n",
    "    for fold in range(10):\n",
    "        model_dict_val = {}\n",
    "        model_dict_test = {}\n",
    "        X_train, A_train, Y_train, X_val, A_val, Y_val, X_test, A_test, Y_test = load_split(\n",
    "            dataset_name, fold, seed\n",
    "        )\n",
    "\n",
    "\n",
    "        models_files_1 = glob(f\"{path_1}/{dataset_name}/{fold}/*.pkl\")\n",
    "        models_files_1 = [file for file in models_files_1 if \"study\" not in file]\n",
    "        models_files_2 = glob(f\"{path_2}/{dataset_name}/{fold}/*.pkl\")\n",
    "        models_files_2 = [file for file in models_files_2 if \"study\" not in file]\n",
    "        models_files = models_files_1 + models_files_2\n",
    "\n",
    "\n",
    "        for file in models_files:\n",
    "            model = joblib.load(file)\n",
    "            model_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "            if \"unaware\" in file:\n",
    "                model_name = model_name + \"_unaware\"\n",
    "            else:\n",
    "                model_name = model_name + \"_aware\"\n",
    "            Y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "            threshold = training.ks_threshold(Y_train, Y_train_pred)\n",
    "            Y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "            Y_val_pred = (Y_val_pred > threshold).astype(int)\n",
    "            model_dict_val[model_name] = Y_val_pred\n",
    "            Y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "            Y_test_pred = (Y_test_pred > threshold).astype(int)\n",
    "            model_dict_test[model_name] = Y_test_pred\n",
    "\n",
    "        metrics_folds_val.append(\n",
    "            evaluate.get_fairness_metrics(model_dict_val, X_val, Y_val, A_val, benefit_class = 0)\n",
    "        )\n",
    "        metrics_folds_test.append(\n",
    "            evaluate.get_fairness_metrics(model_dict_test, X_test, Y_test, A_test, benefit_class = 0)\n",
    "        )\n",
    "        metrics_folds_test[-1][\"fold\"] = fold\n",
    "\n",
    "\n",
    "    metrics_val = pd.concat(metrics_folds_val)    \n",
    "    metrics_test = pd.concat(metrics_folds_test)\n",
    "        \n",
    "    return metrics_val, metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thr_helper:\n",
    "    def __init__(self, model, sensitive_features):\n",
    "        self.model = model\n",
    "        self.sensitive_features = sensitive_features\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(\n",
    "            X, sensitive_features=self.sensitive_features\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {}\n",
    "for dataset in [\"german\", \"taiwan\", \"homecredit\"]:\n",
    "    _, t = summarize_fair_baselines(dataset, 0)\n",
    "    test_metrics[dataset] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german\n",
      "                            model balanced_accuracy               EOD               DPD              APVD\n",
      "0            LGBMClassifier_aware   & 0.632 ± 0.041     & 0.09 ± 0.06   & 0.066 ± 0.037    & 0.13 ± 0.031\n",
      "1          LGBMClassifier_unaware   & 0.656 ± 0.033   & 0.041 ± 0.035   & 0.048 ± 0.025   & 0.103 ± 0.041\n",
      "2        LogisticRegression_aware   & 0.697 ± 0.024    & 0.193 ± 0.06   & 0.124 ± 0.055   & 0.137 ± 0.026\n",
      "3      LogisticRegression_unaware   & 0.701 ± 0.014   & 0.073 ± 0.037    & 0.041 ± 0.03   & 0.094 ± 0.028\n",
      "4             MLPClassifier_aware   & 0.672 ± 0.049   & 0.177 ± 0.169   & 0.145 ± 0.147    & 0.132 ± 0.04\n",
      "5           MLPClassifier_unaware   & 0.696 ± 0.033   & 0.066 ± 0.058   & 0.086 ± 0.047   & 0.088 ± 0.025\n",
      "6    RandomForestClassifier_aware   & 0.672 ± 0.027    & 0.181 ± 0.07   & 0.103 ± 0.065   & 0.132 ± 0.024\n",
      "7  RandomForestClassifier_unaware   & 0.688 ± 0.025    & 0.12 ± 0.056   & 0.067 ± 0.044   & 0.112 ± 0.018\n",
      "\n",
      "\n",
      "\n",
      "taiwan\n",
      "                            model balanced_accuracy               EOD               DPD              APVD\n",
      "0            LGBMClassifier_aware   & 0.721 ± 0.002   & 0.049 ± 0.007   & 0.072 ± 0.007   & 0.025 ± 0.003\n",
      "1          LGBMClassifier_unaware   & 0.721 ± 0.004   & 0.023 ± 0.006   & 0.045 ± 0.006   & 0.036 ± 0.002\n",
      "2        LogisticRegression_aware   & 0.709 ± 0.001    & 0.051 ± 0.02   & 0.078 ± 0.019   & 0.022 ± 0.009\n",
      "3      LogisticRegression_unaware   & 0.709 ± 0.001   & 0.018 ± 0.002   & 0.044 ± 0.002    & 0.04 ± 0.001\n",
      "4             MLPClassifier_aware   & 0.713 ± 0.003   & 0.074 ± 0.016   & 0.099 ± 0.015   & 0.013 ± 0.005\n",
      "5           MLPClassifier_unaware   & 0.711 ± 0.004   & 0.026 ± 0.005    & 0.05 ± 0.004   & 0.035 ± 0.003\n",
      "6    RandomForestClassifier_aware   & 0.721 ± 0.004   & 0.032 ± 0.007   & 0.052 ± 0.005   & 0.033 ± 0.002\n",
      "7  RandomForestClassifier_unaware    & 0.72 ± 0.003    & 0.02 ± 0.006   & 0.041 ± 0.006   & 0.037 ± 0.003\n",
      "\n",
      "\n",
      "\n",
      "homecredit\n",
      "                            model balanced_accuracy               EOD               DPD              APVD\n",
      "0            LGBMClassifier_aware   & 0.688 ± 0.002   & 0.153 ± 0.003   & 0.162 ± 0.003     & 0.011 ± 0.0\n",
      "1          LGBMClassifier_unaware   & 0.687 ± 0.001    & 0.07 ± 0.003    & 0.08 ± 0.003   & 0.023 ± 0.001\n",
      "2        LogisticRegression_aware     & 0.675 ± 0.0   & 0.174 ± 0.003   & 0.182 ± 0.002      & 0.01 ± 0.0\n",
      "3      LogisticRegression_unaware   & 0.671 ± 0.001   & 0.036 ± 0.001   & 0.045 ± 0.001     & 0.028 ± 0.0\n",
      "4             MLPClassifier_aware   & 0.679 ± 0.001   & 0.168 ± 0.007   & 0.178 ± 0.007    & 0.01 ± 0.001\n",
      "5           MLPClassifier_unaware   & 0.674 ± 0.001   & 0.043 ± 0.002   & 0.053 ± 0.002     & 0.027 ± 0.0\n",
      "6    RandomForestClassifier_aware   & 0.679 ± 0.002   & 0.153 ± 0.011   & 0.162 ± 0.012   & 0.012 ± 0.002\n",
      "7  RandomForestClassifier_unaware   & 0.677 ± 0.001   & 0.077 ± 0.002   & 0.086 ± 0.002     & 0.022 ± 0.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in test_metrics:\n",
    "    print(dataset)\n",
    "    print(transform_to_table(test_metrics[dataset][[\"model\", \"balanced_accuracy\", \"EOD\", \"DPD\", \"APVD\"]]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fair Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_fair_methods(dataset_name, goal = 0.01, seed = 0):\n",
    "\n",
    "    baselines_comparison = {\n",
    "        \"DemographicParityClassifier\" : \"LogisticRegression\",\n",
    "        \"EqualOpportunityClassifier\" : \"LogisticRegression\",\n",
    "        \"FairGBMClassifier\" : \"LGBMClassifier\",\n",
    "        \"rw_LogisticRegression\" : \"LogisticRegression\",\n",
    "        \"rw_MLPClassifier\" : \"MLPClassifier\",\n",
    "        \"rw_RandomForestClassifier\" : \"RandomForestClassifier\",\n",
    "        \"rw_LGBMClassifier\" : \"LGBMClassifier\",\n",
    "        \"thr_LogisticRegression\" : \"LogisticRegression\",\n",
    "        \"thr_MLPClassifier\" : \"MLPClassifier\",\n",
    "        \"thr_RandomForestClassifier\" : \"RandomForestClassifier\",\n",
    "        \"thr_LGBMClassifier\" : \"LGBMClassifier\",\n",
    "    }\n",
    "\n",
    "    path_baselines = \"../results/credit_models_unaware\"\n",
    "    path_fair = f\"../results/fair_models\"\n",
    "\n",
    "    metrics_folds_val = []\n",
    "    metrics_folds_test = []\n",
    "    baseline_acc_val = []\n",
    "    baseline_acc_test = []\n",
    "    for fold in range(10):\n",
    "        model_dict_val = {}\n",
    "        model_dict_test = {}\n",
    "        X_train, A_train, Y_train, X_val, A_val, Y_val, X_test, A_test, Y_test = load_split(\n",
    "            dataset_name, fold, seed, unaware = True\n",
    "        )\n",
    "\n",
    "        # get baselines balanced accuracy and eod\n",
    "\n",
    "        models_files = glob(f\"{path_baselines}/{dataset_name}/{fold}/*.pkl\")\n",
    "        models_files = [file for file in models_files if \"study\" not in file]\n",
    "        for file in models_files:\n",
    "            model = joblib.load(file)\n",
    "            model_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            Y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "            threshold = training.ks_threshold(Y_train, Y_train_pred)\n",
    "            Y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "            Y_val_pred = (Y_val_pred > threshold).astype(int)\n",
    "\n",
    "            baseline_acc_val.append(\n",
    "                evaluate.get_fairness_metrics({model_name : Y_val_pred}, X_val, Y_val, A_val, benefit_class = 0)\n",
    "            )\n",
    "            baseline_acc_val[-1][\"fold\"] = fold\n",
    "            \n",
    "            Y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "            Y_test_pred = (Y_test_pred > threshold).astype(int)\n",
    "            baseline_acc_test.append(\n",
    "                evaluate.get_fairness_metrics({model_name : Y_test_pred}, X_test, Y_test, A_test, benefit_class = 0)\n",
    "            )\n",
    "            baseline_acc_test[-1][\"fold\"] = fold\n",
    "\n",
    "\n",
    "        # evaluate fair models\n",
    "        models_files = glob(f\"{path_fair}/{dataset_name}/{fold}/*.pkl\")\n",
    "        models_files = [file for file in models_files if \"study\" not in file]\n",
    "        \n",
    "        for file in models_files:\n",
    "            if file.find(\"thr\") != -1:\n",
    "                continue\n",
    "\n",
    "            if \"DemographicParityClassifier\" in file or \"EqualOpportunityClassifier\" in file:\n",
    "                X_train[\"Z\"] = A_train\n",
    "                X_val[\"Z\"] = A_val\n",
    "                X_test[\"Z\"] = A_test\n",
    "\n",
    "\n",
    "            model = joblib.load(file)\n",
    "            model_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            Y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "            threshold = training.ks_threshold(Y_train, Y_train_pred)\n",
    "            Y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "            Y_val_pred = (Y_val_pred > threshold).astype(int)\n",
    "            model_dict_val[model_name] = Y_val_pred\n",
    "            Y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "            Y_test_pred = (Y_test_pred > threshold).astype(int)\n",
    "            model_dict_test[model_name] = Y_test_pred\n",
    "\n",
    "            if \"DemographicParityClassifier\" in file or \"EqualOpportunityClassifier\" in file:\n",
    "                X_train = X_train.drop(columns = [\"Z\"])\n",
    "                X_val = X_val.drop(columns = [\"Z\"])\n",
    "                X_test = X_test.drop(columns = [\"Z\"])\n",
    "\n",
    "                \n",
    "        metrics_folds_val.append(\n",
    "            evaluate.get_fairness_metrics(model_dict_val, X_val, Y_val, A_val, benefit_class = 0)\n",
    "        )\n",
    "        metrics_folds_val[-1][\"fold\"] = fold\n",
    "        metrics_folds_test.append(\n",
    "            evaluate.get_fairness_metrics(model_dict_test, X_test, Y_test, A_test, benefit_class = 0)\n",
    "        )\n",
    "        metrics_folds_test[-1][\"fold\"] = fold\n",
    "\n",
    "        # Threshold Optimizer needs a different procedure as it does not predict probabilities\n",
    "        for file in models_files:\n",
    "            if file.find(\"thr\") == -1:\n",
    "                continue\n",
    "            \n",
    "            pipeline_preprocess = training.create_pipeline(X_train, Y_train, crit = 4 if dataset == \"homecredit\" else 3)\n",
    "            pipeline_preprocess.fit(X_train, Y_train)\n",
    "            X_val_preprocessed = pipeline_preprocess.transform(X_val)\n",
    "            X_test_preprocessed = pipeline_preprocess.transform(X_test)\n",
    "\n",
    "            model = joblib.load(file)\n",
    "            model_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            thr_opt_helper_val = Thr_helper(model, A_val)\n",
    "            Y_val_pred = thr_opt_helper_val.predict(X_val_preprocessed)\n",
    "            thr_opt_helper_test = Thr_helper(model, A_test)\n",
    "            Y_test_pred = thr_opt_helper_test.predict(X_test_preprocessed)\n",
    "            \n",
    "            model_dict_val = {model_name : Y_val_pred}\n",
    "            model_dict_test = {model_name : Y_test_pred}\n",
    "            metrics_folds_val.append(\n",
    "                evaluate.get_fairness_metrics(\n",
    "                    model_dict_val, X_val_preprocessed, Y_val, A_val, benefit_class = 0\n",
    "                )\n",
    "            )\n",
    "            metrics_folds_val[-1][\"fold\"] = fold\n",
    "            metrics_folds_test.append(\n",
    "                evaluate.get_fairness_metrics(\n",
    "                    model_dict_test, X_test_preprocessed, Y_test, A_test, benefit_class = 0\n",
    "                )\n",
    "            )\n",
    "            metrics_folds_test[-1][\"fold\"] = fold\n",
    "\n",
    "\n",
    "\n",
    "    metrics_val = pd.concat(metrics_folds_val)\n",
    "    metrics_test = pd.concat(metrics_folds_test)\n",
    "    baseline_acc_val = pd.concat(baseline_acc_val)\n",
    "    baseline_acc_test = pd.concat(baseline_acc_test)\n",
    "\n",
    "    diff_bal_acc = []\n",
    "    diff_eod = []\n",
    "    for i, row in metrics_val.iterrows():\n",
    "        fold = row[\"fold\"]\n",
    "        model = row[\"model\"]\n",
    "        compared_model = baselines_comparison[model]\n",
    "        baseline_acc = baseline_acc_val[\n",
    "            (baseline_acc_val[\"model\"] == compared_model) & (baseline_acc_val[\"fold\"] == fold)\n",
    "        ][\"balanced_accuracy\"].values[0]\n",
    "        baseline_eod = baseline_acc_val[\n",
    "            (baseline_acc_val[\"model\"] == compared_model) & (baseline_acc_val[\"fold\"] == fold)\n",
    "        ][\"EOD\"].values[0]\n",
    "        diff_bal_acc.append(row[\"balanced_accuracy\"] - baseline_acc)\n",
    "        diff_eod.append(abs(row[\"EOD\"]) - abs(baseline_eod))\n",
    "    metrics_val[\"diff_bal_acc\"] = diff_bal_acc\n",
    "    metrics_val[\"diff_eod\"] = diff_eod\n",
    "\n",
    "    diff_bal_acc = []\n",
    "    diff_eod = []\n",
    "    for i, row in metrics_test.iterrows():\n",
    "        fold = row[\"fold\"]\n",
    "        model = row[\"model\"]\n",
    "        compared_model = baselines_comparison[model]\n",
    "        baseline_acc = baseline_acc_test[\n",
    "            (baseline_acc_test[\"model\"] == compared_model) & (baseline_acc_test[\"fold\"] == fold)\n",
    "        ][\"balanced_accuracy\"].values[0]\n",
    "        baseline_eod = baseline_acc_test[\n",
    "            (baseline_acc_test[\"model\"] == compared_model) & (baseline_acc_test[\"fold\"] == fold)\n",
    "        ][\"EOD\"].values[0]\n",
    "        diff_bal_acc.append(row[\"balanced_accuracy\"] - baseline_acc)\n",
    "        diff_eod.append(abs(row[\"EOD\"]) - abs(baseline_eod))\n",
    "    metrics_test[\"diff_bal_acc\"] = diff_bal_acc\n",
    "    metrics_test[\"diff_eod\"] = diff_eod\n",
    "\n",
    "    # invert the order of columns\n",
    "    columns = metrics_val.columns.tolist()\n",
    "    columns.reverse()\n",
    "\n",
    "    return metrics_val[columns], metrics_test[columns]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_goals = {\"german\" : 0.05, \"taiwan\" : 0.01, \"homecredit\" : 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {}\n",
    "for dataset in [\"german\", \"taiwan\", \"homecredit\"]:\n",
    "    _, t = summarize_fair_methods(dataset, fairness_goals[dataset], 0)\n",
    "    test_metrics[dataset] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german\n",
      "                          model       diff_bal_acc           diff_eod balanced_accuracy               EOD               DPD              APVD\n",
      "0   DemographicParityClassifier     & 0.009 ± 0.02    & -0.023 ± 0.05    & 0.71 ± 0.014    & 0.05 ± 0.032    & 0.061 ± 0.05   & 0.065 ± 0.024\n",
      "1    EqualOpportunityClassifier    & 0.007 ± 0.018    & -0.02 ± 0.058    & 0.708 ± 0.02   & 0.053 ± 0.042   & 0.043 ± 0.041    & 0.074 ± 0.02\n",
      "2             FairGBMClassifier   & -0.037 ± 0.079    & 0.016 ± 0.072   & 0.618 ± 0.075   & 0.057 ± 0.067   & 0.052 ± 0.038   & 0.129 ± 0.078\n",
      "3             rw_LGBMClassifier   & -0.035 ± 0.047   & -0.001 ± 0.059    & 0.62 ± 0.069    & 0.04 ± 0.041    & 0.038 ± 0.04   & 0.123 ± 0.041\n",
      "4         rw_LogisticRegression   & -0.012 ± 0.038    & 0.023 ± 0.064   & 0.689 ± 0.033   & 0.097 ± 0.049   & 0.057 ± 0.039   & 0.092 ± 0.018\n",
      "5              rw_MLPClassifier    & -0.036 ± 0.07    & 0.006 ± 0.083   & 0.661 ± 0.068   & 0.071 ± 0.044   & 0.079 ± 0.055    & 0.08 ± 0.029\n",
      "6     rw_RandomForestClassifier   & -0.009 ± 0.038   & -0.032 ± 0.083   & 0.678 ± 0.027   & 0.088 ± 0.048   & 0.045 ± 0.036    & 0.095 ± 0.02\n",
      "7            thr_LGBMClassifier    & 0.002 ± 0.011   & -0.006 ± 0.031   & 0.658 ± 0.026   & 0.035 ± 0.031   & 0.054 ± 0.036    & 0.09 ± 0.021\n",
      "8        thr_LogisticRegression   & -0.006 ± 0.022    & 0.045 ± 0.052   & 0.695 ± 0.016   & 0.118 ± 0.039   & 0.059 ± 0.034   & 0.105 ± 0.011\n",
      "9             thr_MLPClassifier    & -0.003 ± 0.02    & 0.029 ± 0.028    & 0.694 ± 0.03   & 0.095 ± 0.061   & 0.088 ± 0.066   & 0.096 ± 0.031\n",
      "10   thr_RandomForestClassifier   & -0.001 ± 0.015    & -0.03 ± 0.043   & 0.686 ± 0.024    & 0.09 ± 0.044   & 0.039 ± 0.025   & 0.107 ± 0.016\n",
      "\n",
      "\n",
      "\n",
      "taiwan\n",
      "                          model       diff_bal_acc           diff_eod balanced_accuracy               EOD               DPD              APVD\n",
      "0   DemographicParityClassifier   & -0.004 ± 0.004    & -0.01 ± 0.005   & 0.705 ± 0.003   & 0.008 ± 0.005    & 0.03 ± 0.009   & 0.049 ± 0.005\n",
      "1    EqualOpportunityClassifier   & -0.005 ± 0.005   & -0.009 ± 0.006   & 0.704 ± 0.005   & 0.008 ± 0.007   & 0.028 ± 0.011   & 0.049 ± 0.006\n",
      "2             FairGBMClassifier   & -0.015 ± 0.024   & -0.011 ± 0.012   & 0.706 ± 0.022   & 0.012 ± 0.007   & 0.035 ± 0.008   & 0.043 ± 0.005\n",
      "3             rw_LGBMClassifier   & -0.036 ± 0.067    & -0.01 ± 0.011   & 0.685 ± 0.067   & 0.014 ± 0.009   & 0.033 ± 0.014   & 0.041 ± 0.005\n",
      "4         rw_LogisticRegression   & -0.001 ± 0.004      & 0.0 ± 0.003   & 0.708 ± 0.005   & 0.018 ± 0.002   & 0.044 ± 0.002    & 0.04 ± 0.002\n",
      "5              rw_MLPClassifier   & -0.087 ± 0.107    & -0.01 ± 0.016   & 0.624 ± 0.107   & 0.015 ± 0.014   & 0.031 ± 0.027   & 0.036 ± 0.004\n",
      "6     rw_RandomForestClassifier   & -0.003 ± 0.006   & -0.005 ± 0.006   & 0.717 ± 0.005   & 0.016 ± 0.005   & 0.037 ± 0.002   & 0.039 ± 0.003\n",
      "7            thr_LGBMClassifier   & -0.001 ± 0.001   & -0.015 ± 0.008    & 0.72 ± 0.005   & 0.008 ± 0.004   & 0.015 ± 0.005   & 0.048 ± 0.003\n",
      "8        thr_LogisticRegression     & -0.0 ± 0.002   & -0.013 ± 0.004   & 0.709 ± 0.002   & 0.005 ± 0.003   & 0.022 ± 0.003   & 0.052 ± 0.002\n",
      "9             thr_MLPClassifier   & -0.002 ± 0.001    & -0.02 ± 0.008    & 0.71 ± 0.004   & 0.006 ± 0.003   & 0.022 ± 0.006    & 0.05 ± 0.004\n",
      "10   thr_RandomForestClassifier   & -0.001 ± 0.002   & -0.014 ± 0.011   & 0.719 ± 0.003   & 0.006 ± 0.005   & 0.016 ± 0.006   & 0.048 ± 0.003\n",
      "\n",
      "\n",
      "\n",
      "homecredit\n",
      "                        model       diff_bal_acc           diff_eod balanced_accuracy               EOD               DPD              APVD\n",
      "0           FairGBMClassifier    & -0.026 ± 0.01   & -0.029 ± 0.005    & 0.661 ± 0.01   & 0.041 ± 0.006    & 0.05 ± 0.006   & 0.029 ± 0.001\n",
      "1           rw_LGBMClassifier   & -0.001 ± 0.002   & -0.037 ± 0.004   & 0.686 ± 0.001   & 0.033 ± 0.001   & 0.044 ± 0.001     & 0.029 ± 0.0\n",
      "2       rw_LogisticRegression   & -0.001 ± 0.001    & -0.03 ± 0.002   & 0.671 ± 0.001   & 0.007 ± 0.001   & 0.015 ± 0.001     & 0.032 ± 0.0\n",
      "3            rw_MLPClassifier   & -0.001 ± 0.001   & -0.032 ± 0.004   & 0.673 ± 0.001   & 0.011 ± 0.003   & 0.021 ± 0.003   & 0.032 ± 0.001\n",
      "4   rw_RandomForestClassifier   & -0.004 ± 0.008    & -0.03 ± 0.003   & 0.673 ± 0.007   & 0.047 ± 0.002   & 0.056 ± 0.002     & 0.027 ± 0.0\n",
      "5          thr_LGBMClassifier   & -0.003 ± 0.002   & -0.066 ± 0.005   & 0.684 ± 0.002   & 0.004 ± 0.002   & 0.006 ± 0.002   & 0.035 ± 0.001\n",
      "6      thr_LogisticRegression   & -0.002 ± 0.001   & -0.035 ± 0.002   & 0.669 ± 0.001   & 0.002 ± 0.001   & 0.006 ± 0.001     & 0.033 ± 0.0\n",
      "7           thr_MLPClassifier   & -0.003 ± 0.001   & -0.039 ± 0.002   & 0.672 ± 0.001   & 0.004 ± 0.002   & 0.005 ± 0.001   & 0.034 ± 0.001\n",
      "8  thr_RandomForestClassifier   & -0.002 ± 0.001   & -0.075 ± 0.002   & 0.675 ± 0.001   & 0.002 ± 0.001   & 0.007 ± 0.001     & 0.033 ± 0.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in test_metrics:\n",
    "    print(dataset)\n",
    "    print(transform_to_table(test_metrics[dataset][[\"model\", \"diff_bal_acc\", \"diff_eod\", \"balanced_accuracy\", \"EOD\", \"DPD\", \"APVD\"]]))\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
